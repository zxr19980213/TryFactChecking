{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer,BertModel\n",
    "import logging\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sim_Jaccard (str1,str2) :\n",
    "    set1 = set( str1.lower().replace(';',' ').replace(',',' ').replace('.',' ').replace(':',' ').replace('&',' ').\n",
    "               replace('/',' ').replace('\\'',' ').replace('(author)',' ').replace('(joint author)',' ').split() )\n",
    "    set2 = set( str2.lower().replace(';',' ').replace(',',' ').replace('.',' ').replace(':',' ').replace('&',' ').\n",
    "               replace('/',' ').replace('\\'',' ').replace('(author)',' ').replace('(joint author)',' ').split() )\n",
    "    return len(set1&set2)/len(set1|set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#查验Jaccard标签效果，同时发现一些问题下存在全1或全0标签，后期可考虑删除等特殊处理\n",
    "def watch_data (i):\n",
    "    label_row = label.loc[i]\n",
    "    print(label_row['author'])\n",
    "    data_rows = data_golden[data_golden['isbn']==label_row['isbn']]\n",
    "    for index,row in data_rows.iterrows():\n",
    "        print(sim_Jaccard(label_row['author'],row['author']))\n",
    "        \n",
    "# 5025个1标签 -> 5619个1标签 说明某些字符的存在客观上拉低了标签的准确性\n",
    "sum(data_golden['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of all data :  2245349\n",
      "length of all label :  407\n",
      "length of data_golden :  11220\n",
      "num of 1 labels :  5609\n",
      "num of sellers :  1774\n",
      "num of books :  407\n"
     ]
    }
   ],
   "source": [
    "#读取并剪切新数据集\n",
    "\n",
    "#读取文件夹中所有数据文件\n",
    "path = './DataSet/vldbBook/book'\n",
    "files = os.listdir(path)\n",
    "data = pd.DataFrame()\n",
    "for file in files:\n",
    "    if 'book_detail' in file:\n",
    "        data = data.append(pd.read_table(path+'/'+file,low_memory=False))\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "data = data.drop(columns=['ISBN_13','location','seller_link','seller_date','publisher','publish_date','binding','book_condition',\n",
    "                          'illustrator','dust_jacket_condition','signed','edition','price','link','used','small_cate'])\n",
    "data.rename(columns={'ISBN_10':'isbn','title':'name','authors':'author','seller':'source'},inplace=True)\n",
    "data.drop_duplicates(subset=['isbn','source'],inplace=True)\n",
    "data['author'].fillna('Not Available', inplace=True)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "print(\"length of all data : \",len(data))\n",
    "#data有2245349行\n",
    "\n",
    "label = pd.read_table('./DataSet/vldbBook/book_truth.txt',low_memory=False,index_col=0)\n",
    "label.rename(columns={'isbn_10':'isbn','authors_truth':'author'},inplace=True)\n",
    "print(\"length of all label : \",len(label))\n",
    "#label有407行，以isbn为index，以author为value\n",
    "\n",
    "#提取data中有标签的数据，加上标签构成data_golden\n",
    "data_golden = pd.DataFrame()\n",
    "for index1,row1 in label.iterrows():\n",
    "    str1 = row1['author']\n",
    "    if(type(str1)==float):\n",
    "        print(index1,row1)\n",
    "        break\n",
    "    data_slice = pd.DataFrame(data[data['isbn']==index1])\n",
    "    data_slice['label'] = None\n",
    "    for index2,row2 in data_slice.iterrows():\n",
    "        str2 = row2['author']\n",
    "        if(type(str2)==float):\n",
    "            print(index2,row2)\n",
    "            break\n",
    "        data_slice.loc[index2,'label'] = ( sim_Jaccard(str1,str2)>=0.8 )   \n",
    "        #这里修改成功了？离谱\n",
    "        #if(sim_Jaccard(str1,str2)>=0.8):\n",
    "        #    row2['label'] = True\n",
    "        #else:\n",
    "        #    row2['label'] = False\n",
    "    data_golden = data_golden.append(data_slice)\n",
    "data_golden.reset_index(inplace=True,drop=True)\n",
    "print('length of data_golden : ',len(data_golden))\n",
    "#data_golden去冗余后剩余11220行4列，来自1774 source、407 book，共有5609个1标签\n",
    "print('num of 1 labels : ',sum(data_golden['label']))\n",
    "seller_list = data_golden['source'].drop_duplicates().reset_index(drop=True)\n",
    "book_list = data_golden['isbn'].drop_duplicates().reset_index(drop=True)\n",
    "print(\"num of sellers : \",len(seller_list))\n",
    "print(\"num of books : \",len(book_list))\n",
    "\n",
    "\n",
    "#保存中间结果\n",
    "data_golden.to_csv( './DataSet/vldbBook/data_golden/data_golden.txt' , sep='\\t' , index=False )\n",
    "\n",
    "#读取中间结果\n",
    "#data_golden=pd.read_csv( './DataSet/vldbBook/data_golden2/data_golden.txt' , sep='\\t' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\zxr\\.pytorch_pretrained_bert\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at C:\\Users\\zxr\\.pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file C:\\Users\\zxr\\.pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir C:\\Users\\zxr\\AppData\\Local\\Temp\\tmp65rqas27\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#加载bert模型和分词器\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#尝试使用bert分别提取各列语义特征作为GCN的输入feature\n",
    "def feedColumnIntoBert(feature):\n",
    "    for i in range(0,len(feature)):\n",
    "        feature.loc[i] = '[CLS] '+str(feature.loc[i])+' [SEP]'\n",
    "        feature.loc[i] = tokenizer.tokenize(feature.loc[i])\n",
    "        feature.loc[i] = tokenizer.convert_tokens_to_ids(feature.loc[i])\n",
    "        feature.loc[i] = torch.tensor([feature.loc[i]])\n",
    "        with torch.no_grad():\n",
    "            encoded_layers,_ = model(feature.loc[i])\n",
    "        feature.loc[i] = encoded_layers[-1][0][0]+encoded_layers[-2][0][0]+encoded_layers[-3][0][0]+encoded_layers[-4][0][0]\n",
    "    return feature\n",
    "feature1 = feedColumnIntoBert(pd.Series(data_golden['source']).copy())\n",
    "feature2 = feedColumnIntoBert(pd.Series(data_golden['name']).copy())\n",
    "feature3 = feedColumnIntoBert(pd.Series(data_golden['author']).copy())\n",
    "for i in range(0,len(feature1)):\n",
    "    feature1.loc[i] = torch.cat((feature1.loc[i],feature2.loc[i],feature3.loc[i]))\n",
    "#torch.save( feature1 , './DataSet/vldbBook/data_golden/bertEncodeFull.pt' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#根据dataframe构建DGL图，添加如此多的边是否会起到恰当作用是我目前考虑的一个问题；\n",
    "#传播会使得feature膨胀，是否需要重新设计GCN中的传播函数？\n",
    "\n",
    "#1.添加一个连边限制，同source且同big_cate可连边，仅同source不可连边\n",
    "#2.每一个节点上增加一个自环\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl import DGLGraph\n",
    "\n",
    "import pickle\n",
    "\n",
    "def generate_DGLGraph(df):\n",
    "    g = DGLGraph()\n",
    "    g.add_nodes(df.shape[0])\n",
    "    \n",
    "    source_list = df['source'].drop_duplicates().reset_index(drop=True)\n",
    "    isbn_list = df['isbn'].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    for index,row in df.iterrows():\n",
    "        g.add_edge(index,index)\n",
    "    \n",
    "    for index,value in source_list.iteritems():\n",
    "        df_slice = df[df['source']==value]\n",
    "    #for k in range(0,1):\n",
    "    #    df_slice = df[df['source']==source_list.loc[k]]\n",
    "        for i in range(0,len(df_slice)):\n",
    "            for j in range(i+1,len(df_slice)):\n",
    "                if(df_slice.iloc[i]['big_cate']==df_slice.iloc[j]['big_cate']):\n",
    "                    g.add_edge(df_slice.iloc[i].name,df_slice.iloc[j].name)\n",
    "                    g.add_edge(df_slice.iloc[j].name,df_slice.iloc[i].name)\n",
    "\n",
    "    for index,value in isbn_list.iteritems():\n",
    "        df_slice = df[df['isbn']==value]\n",
    "    #for k in range(0,1):\n",
    "    #    df_slice = df[df['isbn']==isbn_list.loc[k]]\n",
    "        for i in range(0,len(df_slice)):\n",
    "            for j in range(i+1,len(df_slice)):\n",
    "                if( sim_Jaccard( df_slice.iloc[i]['author'] , df_slice.iloc[j]['author'] ) >= 0.8 ):\n",
    "                    g.add_edge(df_slice.iloc[i].name,df_slice.iloc[j].name)\n",
    "                    g.add_edge(df_slice.iloc[j].name,df_slice.iloc[i].name)\n",
    "\n",
    "    return g\n",
    "\n",
    "#图中共有979230条有向边 -> 463000 -> 474220\n",
    "g = generate_DGLGraph(data_golden)\n",
    "\n",
    "#使用pickle将图存储在文件中以便复用\n",
    "file = open('./DataSet/vldbBook/data_golden/graph.pickle', 'wb')\n",
    "pickle.dump(g, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([474220])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.edges()[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_gpu]",
   "language": "python",
   "name": "conda-env-pytorch_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
