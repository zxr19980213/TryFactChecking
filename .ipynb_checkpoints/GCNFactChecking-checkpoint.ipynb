{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl import DGLGraph\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#来自DGLGraph tutorial\n",
    "class NodeApplyModule(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        if self.activation is not None:\n",
    "            h = self.activation(h)\n",
    "        return {'h' : h}\n",
    "\n",
    "gcn_msg = fn.copy_src(src='h', out='m')\n",
    "gcn_reduce = fn.sum(msg='m', out='h')\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(gcn_msg, gcn_reduce)\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.gcn1 = GCN(768, 32, F.relu)\n",
    "        self.gcn2 = GCN(32, 2, None)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = self.gcn1(g, features)\n",
    "        x = self.gcn2(g, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读取DealWithData中golden数据\n",
    "data_golden = pd.read_csv( './DataSet/book/golden/claims_golden.txt' , sep='\\t' )\n",
    "data_golden['encode'] = torch.load('./DataSet/book/golden/claims_golden_encode.pt')\n",
    "GoldenLabel = pd.read_table(\"./DataSet/book/book_golden.txt\" , sep='\\t' , header=None , names=['isbn','author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#随意点划分训练集和测试集，直觉要根据'isbn'划分\n",
    "data_train = pd.DataFrame(data_golden.drop(data_golden.index,inplace=False))\n",
    "data_test = pd.DataFrame(data_golden.drop(data_golden.index,inplace=False))\n",
    "for i in range(0,int(len(GoldenLabel)/2+1)):\n",
    "    data_trainSlice = data_golden[data_golden['isbn']==GoldenLabel.loc[i]['isbn']]\n",
    "    data_train = data_train.append(data_trainSlice)\n",
    "for i in range(int(len(GoldenLabel)/2+1),len(GoldenLabel)):\n",
    "    data_testSlice = data_golden[data_golden['isbn']==GoldenLabel.loc[i]['isbn']]\n",
    "    data_test = data_test.append(data_testSlice)\n",
    "data_train.reset_index(drop=True,inplace=True)\n",
    "data_test.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#构建图结构函数，根据训练集和测试集连边\n",
    "#此处可优化复杂度？n**2/2 -> kn\n",
    "#此处可根据'book_name'相似度进一步加边\n",
    "def generate_DGLGraph(df):\n",
    "    edge_norm = []\n",
    "    g = DGLGraph()\n",
    "    g.add_nodes(df.shape[0])\n",
    "    for i in range(0,len(df)):\n",
    "        for j in range(i+1,len(df)):\n",
    "            if (df.loc[i][\"source\"]==df.loc[j][\"source\"]):\n",
    "                g.add_edge(i,j)\n",
    "                #edge_norm.append(1.0)\n",
    "                g.add_edge(j,i)\n",
    "                #edge_norm.append(1.0)\n",
    "            elif (df.loc[i][\"isbn\"]==df.loc[j][\"isbn\"]):\n",
    "                str1 = df.loc[i]['author']\n",
    "                str2 = df.loc[j]['author']\n",
    "                #print(str1,str2,difflib.SequenceMatcher(None,str1,str2).quick_ratio())\n",
    "                if ( difflib.SequenceMatcher(None,str1,str2).quick_ratio()>0.8 ):\n",
    "                    g.add_edge(i,j)\n",
    "                    #edge_norm.append(1.0)\n",
    "                    g.add_edge(j,i)\n",
    "                    #edge_norm.append(1.0)\n",
    "    #edge_norm = torch.Tensor(edge_norm).unsqueeze(1)\n",
    "    #g.edata.update({ 'norm': edge_norm })\n",
    "    return g\n",
    "\n",
    "graph_train = generate_DGLGraph(data_train)\n",
    "graph_test = generate_DGLGraph(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#存储图结构\n",
    "import pickle\n",
    "file = open('./DataSet/book/golden/graph_train.pickle', 'wb')\n",
    "pickle.dump(graph_train, file)\n",
    "file.close()\n",
    "file = open('./DataSet/book/golden/graph_test.pickle', 'wb')\n",
    "pickle.dump(graph_test, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加载图结构\n",
    "with open('./DataSet/book/golden/graph_train.pickle', 'rb') as file:\n",
    "    graph_train =pickle.load(file)\n",
    "with open('./DataSet/book/golden/graph_test.pickle', 'rb') as file:\n",
    "    graph_test =pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#存储和加载图结构的失败尝试\n",
    "'''\n",
    "graph_train_netx = graph_train.to_networkx()\n",
    "graph_test_netx = graph_test.to_networkx()\n",
    "nx.write_gexf(graph_train_netx,'./DataSet/book/golden/train_graph.gexf')\n",
    "nx.write_gexf(graph_test_netx,'./DataSet/book/golden/test_graph.gexf')\n",
    "\n",
    "graph_train_netx = nx.read_gexf('./DataSet/book/golden/train_graph.gexf')\n",
    "graph_test_netx = nx.read_gexf('./DataSet/book/golden/test_graph.gexf')\n",
    "\n",
    "graph_train2 = DGLGraph(graph_train_netx)\n",
    "#graph_train2.from_networkx(graph_train_netx)\n",
    "graph_test2 = DGLGraph(graph_test_netx)\n",
    "#graph_test2.from_networkx(graph_test_netx)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_featureNlabel(df):\n",
    "    a = torch.zeros(1,768)\n",
    "    c = torch.zeros(1)\n",
    "    for _,row in df.iterrows():\n",
    "        b = row['encode'].reshape([-1,768])\n",
    "        a = torch.cat((a,b),0)\n",
    "        if(row['label']):\n",
    "            d = torch.ones(1)\n",
    "        else:\n",
    "            d = torch.zeros(1)\n",
    "        c = torch.cat((c,d),-1)\n",
    "    return a[1:,:],c[1:].long()\n",
    "train_feature,train_label = extract_featureNlabel(data_train)\n",
    "test_feature,test_label = extract_featureNlabel(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 2115.1045 | Time(s) 0.3333\n",
      "tensor([[     0.0000,   -474.9843],\n",
      "        [     0.0000,   -490.0583],\n",
      "        [     0.0000,    -38.5609],\n",
      "        ...,\n",
      "        [     0.0000, -10261.2266],\n",
      "        [     0.0000,  -6876.0869],\n",
      "        [     0.0000,  -6765.7183]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00001 | Loss 1957.4723 | Time(s) 0.3261\n",
      "tensor([[    0.0000,  -435.8128],\n",
      "        [    0.0000,  -449.5630],\n",
      "        [    0.0000,   -35.7825],\n",
      "        ...,\n",
      "        [    0.0000, -9625.3809],\n",
      "        [    0.0000, -6446.9688],\n",
      "        [    0.0000, -6246.3188]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00002 | Loss 1799.9904 | Time(s) 0.3169\n",
      "tensor([[    0.0000,  -397.2705],\n",
      "        [    0.0000,  -409.6799],\n",
      "        [    0.0000,   -32.9936],\n",
      "        ...,\n",
      "        [    0.0000, -8982.5068],\n",
      "        [    0.0000, -6018.7417],\n",
      "        [    0.0000, -5709.8628]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00003 | Loss 1646.5574 | Time(s) 0.3152\n",
      "tensor([[    0.0000,  -361.9127],\n",
      "        [    0.0000,  -373.0261],\n",
      "        [    0.0000,   -30.2913],\n",
      "        ...,\n",
      "        [    0.0000, -8325.2676],\n",
      "        [    0.0000, -5583.0039],\n",
      "        [    0.0000, -5167.5562]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00004 | Loss 1497.7350 | Time(s) 0.3130\n",
      "tensor([[    0.0000,  -327.2256],\n",
      "        [    0.0000,  -337.0922],\n",
      "        [    0.0000,   -27.7917],\n",
      "        ...,\n",
      "        [    0.0000, -7644.5610],\n",
      "        [    0.0000, -5124.0669],\n",
      "        [    0.0000, -4628.5347]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00005 | Loss 1350.9921 | Time(s) 0.3116\n",
      "tensor([[    0.0000,  -294.0170],\n",
      "        [    0.0000,  -302.6516],\n",
      "        [    0.0000,   -25.3826],\n",
      "        ...,\n",
      "        [    0.0000, -6966.2778],\n",
      "        [    0.0000, -4665.9160],\n",
      "        [    0.0000, -4157.0186]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00006 | Loss 1210.5203 | Time(s) 0.3143\n",
      "tensor([[    0.0000,  -265.6672],\n",
      "        [    0.0000,  -273.0807],\n",
      "        [    0.0000,   -23.2679],\n",
      "        ...,\n",
      "        [    0.0000, -6298.9502],\n",
      "        [    0.0000, -4211.6436],\n",
      "        [    0.0000, -3716.0588]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00007 | Loss 1078.8268 | Time(s) 0.3133\n",
      "tensor([[    0.0000,  -239.3440],\n",
      "        [    0.0000,  -245.6180],\n",
      "        [    0.0000,   -21.1262],\n",
      "        ...,\n",
      "        [    0.0000, -5639.3496],\n",
      "        [    0.0000, -3763.8340],\n",
      "        [    0.0000, -3283.2976]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00008 | Loss 948.9244 | Time(s) 0.3122\n",
      "tensor([[    0.0000,  -212.4628],\n",
      "        [    0.0000,  -217.6944],\n",
      "        [    0.0000,   -18.9683],\n",
      "        ...,\n",
      "        [    0.0000, -4987.7803],\n",
      "        [    0.0000, -3322.2803],\n",
      "        [    0.0000, -2854.5393]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00009 | Loss 820.1699 | Time(s) 0.3100\n",
      "tensor([[    0.0000,  -185.0839],\n",
      "        [    0.0000,  -189.4261],\n",
      "        [    0.0000,   -16.8030],\n",
      "        ...,\n",
      "        [    0.0000, -4345.2852],\n",
      "        [    0.0000, -2885.3628],\n",
      "        [    0.0000, -2434.8027]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00010 | Loss 692.7736 | Time(s) 0.3091\n",
      "tensor([[ 0.0000e+00, -1.5724e+02],\n",
      "        [ 0.0000e+00, -1.6068e+02],\n",
      "        [-3.5763e-07, -1.4699e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -3.7147e+03],\n",
      "        [ 0.0000e+00, -2.4544e+03],\n",
      "        [ 0.0000e+00, -2.0537e+03]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00011 | Loss 566.3432 | Time(s) 0.3086\n",
      "tensor([[ 0.0000e+00, -1.2896e+02],\n",
      "        [ 0.0000e+00, -1.3149e+02],\n",
      "        [-3.3379e-06, -1.2615e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -3.0956e+03],\n",
      "        [ 0.0000e+00, -2.0340e+03],\n",
      "        [ 0.0000e+00, -1.6766e+03]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00012 | Loss 441.8272 | Time(s) 0.3073\n",
      "tensor([[ 0.0000e+00, -1.0023e+02],\n",
      "        [ 0.0000e+00, -1.0183e+02],\n",
      "        [-2.5868e-05, -1.0563e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -2.5302e+03],\n",
      "        [ 0.0000e+00, -1.6635e+03],\n",
      "        [ 0.0000e+00, -1.2976e+03]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00013 | Loss 318.1707 | Time(s) 0.3086\n",
      "tensor([[ 0.0000e+00, -7.1036e+01],\n",
      "        [ 0.0000e+00, -7.1748e+01],\n",
      "        [-1.9107e-04, -8.5631e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.9785e+03],\n",
      "        [ 0.0000e+00, -1.3049e+03],\n",
      "        [ 0.0000e+00, -9.1324e+02]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00014 | Loss 195.3486 | Time(s) 0.3104\n",
      "tensor([[ 0.0000e+00, -4.1362e+01],\n",
      "        [ 0.0000e+00, -4.1189e+01],\n",
      "        [-1.4145e-03, -6.5617e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.4446e+03],\n",
      "        [ 0.0000e+00, -9.4720e+02],\n",
      "        [ 0.0000e+00, -5.2399e+02]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00015 | Loss 99.9200 | Time(s) 0.3100\n",
      "tensor([[-1.3828e-05, -1.1189e+01],\n",
      "        [-4.0530e-05, -1.0113e+01],\n",
      "        [-1.0878e-02, -4.5265e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -9.2235e+02],\n",
      "        [ 0.0000e+00, -5.9033e+02],\n",
      "        [ 0.0000e+00, -1.2952e+02]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00016 | Loss 79.2651 | Time(s) 0.3098\n",
      "tensor([[-1.8273e+01,  0.0000e+00],\n",
      "        [-2.0234e+01,  0.0000e+00],\n",
      "        [-7.7820e-02, -2.5920e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -4.2349e+02],\n",
      "        [ 0.0000e+00, -2.4977e+02],\n",
      "        [-2.5488e+02,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00017 | Loss 82.4478 | Time(s) 0.3099\n",
      "tensor([[-4.5300e+01,  0.0000e+00],\n",
      "        [-4.8065e+01,  0.0000e+00],\n",
      "        [-4.0832e-01, -1.0929e+00],\n",
      "        ...,\n",
      "        [-2.4159e+01,  0.0000e+00],\n",
      "        [-5.2649e+01,  0.0000e+00],\n",
      "        [-6.0437e+02,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00018 | Loss 107.8944 | Time(s) 0.3096\n",
      "tensor([[-6.8818e+01,  0.0000e+00],\n",
      "        [-7.2278e+01,  0.0000e+00],\n",
      "        [-1.2373e+00, -3.4271e-01],\n",
      "        ...,\n",
      "        [-4.1318e+02,  0.0000e+00],\n",
      "        [-3.1303e+02,  0.0000e+00],\n",
      "        [-9.0698e+02,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00019 | Loss 145.5819 | Time(s) 0.3107\n",
      "tensor([[-8.8496e+01,  0.0000e+00],\n",
      "        [-9.2535e+01,  0.0000e+00],\n",
      "        [-2.3075e+00, -1.0481e-01],\n",
      "        ...,\n",
      "        [-7.3986e+02,  0.0000e+00],\n",
      "        [-5.2912e+02,  0.0000e+00],\n",
      "        [-1.1580e+03,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00020 | Loss 180.7941 | Time(s) 0.3111\n",
      "tensor([[-1.0279e+02,  0.0000e+00],\n",
      "        [-1.0725e+02,  0.0000e+00],\n",
      "        [-3.1828e+00, -4.2356e-02],\n",
      "        ...,\n",
      "        [-9.7838e+02,  0.0000e+00],\n",
      "        [-6.8494e+02,  0.0000e+00],\n",
      "        [-1.3384e+03,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00021 | Loss 203.9666 | Time(s) 0.3103\n",
      "tensor([[-1.1231e+02,  0.0000e+00],\n",
      "        [-1.1704e+02,  0.0000e+00],\n",
      "        [-3.7705e+00, -2.3310e-02],\n",
      "        ...,\n",
      "        [-1.1387e+03,  0.0000e+00],\n",
      "        [-7.8559e+02,  0.0000e+00],\n",
      "        [-1.4564e+03,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00022 | Loss 216.7501 | Time(s) 0.3105\n",
      "tensor([[-1.1766e+02,  0.0000e+00],\n",
      "        [-1.2254e+02,  0.0000e+00],\n",
      "        [-4.0820e+00, -1.7018e-02],\n",
      "        ...,\n",
      "        [-1.2325e+03,  0.0000e+00],\n",
      "        [-8.4012e+02,  0.0000e+00],\n",
      "        [-1.5202e+03,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00023 | Loss 220.2285 | Time(s) 0.3109\n",
      "tensor([[-1.1933e+02,  0.0000e+00],\n",
      "        [-1.2425e+02,  0.0000e+00],\n",
      "        [-4.1459e+00, -1.5956e-02],\n",
      "        ...,\n",
      "        [-1.2651e+03,  0.0000e+00],\n",
      "        [-8.5392e+02,  0.0000e+00],\n",
      "        [-1.5359e+03,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00024 | Loss 215.3544 | Time(s) 0.3108\n",
      "tensor([[-1.1777e+02,  0.0000e+00],\n",
      "        [-1.2263e+02,  0.0000e+00],\n",
      "        [-3.9933e+00, -1.8610e-02],\n",
      "        ...,\n",
      "        [-1.2416e+03,  0.0000e+00],\n",
      "        [-8.3189e+02,  0.0000e+00],\n",
      "        [-1.5093e+03,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00025 | Loss 203.2748 | Time(s) 0.3108\n",
      "tensor([[-1.1335e+02,  0.0000e+00],\n",
      "        [-1.1806e+02,  0.0000e+00],\n",
      "        [-3.6540e+00, -2.6229e-02],\n",
      "        ...,\n",
      "        [-1.1669e+03,  0.0000e+00],\n",
      "        [-7.7826e+02,  0.0000e+00],\n",
      "        [-1.4452e+03,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00026 | Loss 184.9557 | Time(s) 0.3103\n",
      "tensor([[-1.0640e+02,  0.0000e+00],\n",
      "        [-1.1090e+02,  0.0000e+00],\n",
      "        [-3.1549e+00, -4.3577e-02],\n",
      "        ...,\n",
      "        [-1.0496e+03,  0.0000e+00],\n",
      "        [-6.9702e+02,  0.0000e+00],\n",
      "        [-1.3483e+03,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00027 | Loss 161.0692 | Time(s) 0.3098\n",
      "tensor([[-9.7227e+01,  0.0000e+00],\n",
      "        [-1.0143e+02,  0.0000e+00],\n",
      "        [-2.5276e+00, -8.3222e-02],\n",
      "        ...,\n",
      "        [-8.9673e+02,  0.0000e+00],\n",
      "        [-5.9119e+02,  0.0000e+00],\n",
      "        [-1.2221e+03,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00028 | Loss 133.3254 | Time(s) 0.3092\n",
      "tensor([[-8.6090e+01,  0.0000e+00],\n",
      "        [-8.9952e+01,  0.0000e+00],\n",
      "        [-1.8235e+00, -1.7610e-01],\n",
      "        ...,\n",
      "        [-7.1224e+02,  0.0000e+00],\n",
      "        [-4.6374e+02,  0.0000e+00],\n",
      "        [-1.0699e+03,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00029 | Loss 109.1926 | Time(s) 0.3092\n",
      "tensor([[-7.3531e+01,  0.0000e+00],\n",
      "        [-7.7008e+01,  0.0000e+00],\n",
      "        [-1.1450e+00, -3.8305e-01],\n",
      "        ...,\n",
      "        [-5.0501e+02,  0.0000e+00],\n",
      "        [-3.2068e+02,  0.0000e+00],\n",
      "        [-8.9904e+02,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00030 | Loss 93.1483 | Time(s) 0.3098\n",
      "tensor([[-6.0620e+01,  0.0000e+00],\n",
      "        [-6.3702e+01,  0.0000e+00],\n",
      "        [-6.2151e-01, -7.7031e-01],\n",
      "        ...,\n",
      "        [-2.9174e+02,  0.0000e+00],\n",
      "        [-1.7348e+02,  0.0000e+00],\n",
      "        [-7.2336e+02,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00031 | Loss 79.3669 | Time(s) 0.3097\n",
      "tensor([[-4.7625e+01,  0.0000e+00],\n",
      "        [-5.0310e+01,  0.0000e+00],\n",
      "        [-2.9619e-01, -1.3612e+00],\n",
      "        ...,\n",
      "        [-7.6431e+01,  0.0000e+00],\n",
      "        [-2.5021e+01,  0.0000e+00],\n",
      "        [-5.4600e+02,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00032 | Loss 73.0629 | Time(s) 0.3096\n",
      "tensor([[-3.4753e+01,  0.0000e+00],\n",
      "        [-3.7044e+01,  0.0000e+00],\n",
      "        [-1.3009e-01, -2.1039e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.3696e+02],\n",
      "        [ 0.0000e+00, -1.2184e+02],\n",
      "        [-3.6995e+02,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00033 | Loss 72.6528 | Time(s) 0.3099\n",
      "tensor([[-2.2820e+01,  0.0000e+00],\n",
      "        [-2.4744e+01,  0.0000e+00],\n",
      "        [-5.8143e-02, -2.8738e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -3.3312e+02],\n",
      "        [ 0.0000e+00, -2.5651e+02],\n",
      "        [-2.0604e+02,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00034 | Loss 74.9012 | Time(s) 0.3098\n",
      "tensor([[-1.2169e+01, -5.2452e-06],\n",
      "        [-1.3765e+01, -1.0729e-06],\n",
      "        [-2.7847e-02, -3.5949e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -5.0716e+02],\n",
      "        [ 0.0000e+00, -3.7586e+02],\n",
      "        [-5.8725e+01,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00035 | Loss 80.4174 | Time(s) 0.3104\n",
      "tensor([[-3.0108e+00, -5.0505e-02],\n",
      "        [-4.2835e+00, -1.3891e-02],\n",
      "        [-1.4600e-02, -4.2340e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -6.5712e+02],\n",
      "        [ 0.0000e+00, -4.7869e+02],\n",
      "        [ 0.0000e+00, -6.9797e+01]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00036 | Loss 89.3202 | Time(s) 0.3100\n",
      "tensor([[-1.7047e-02, -4.0803e+00],\n",
      "        [-4.9714e-02, -3.0262e+00],\n",
      "        [-8.8151e-03, -4.7357e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -7.7191e+02],\n",
      "        [ 0.0000e+00, -5.5741e+02],\n",
      "        [ 0.0000e+00, -1.6893e+02]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00037 | Loss 97.2066 | Time(s) 0.3101\n",
      "tensor([[-3.0060e-04, -8.1100e+00],\n",
      "        [-7.7992e-04, -7.1567e+00],\n",
      "        [-6.5157e-03, -5.0368e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -8.3787e+02],\n",
      "        [ 0.0000e+00, -6.0281e+02],\n",
      "        [ 0.0000e+00, -2.2861e+02]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00038 | Loss 99.5611 | Time(s) 0.3099\n",
      "tensor([[-9.7866e-05, -9.2323e+00],\n",
      "        [-2.4304e-04, -8.3226e+00],\n",
      "        [-5.8899e-03, -5.1375e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -8.5574e+02],\n",
      "        [ 0.0000e+00, -6.1541e+02],\n",
      "        [ 0.0000e+00, -2.4967e+02]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00039 | Loss 96.5589 | Time(s) 0.3103\n",
      "tensor([[-4.5945e-04, -7.6858e+00],\n",
      "        [-1.1835e-03, -6.7398e+00],\n",
      "        [-6.3925e-03, -5.0558e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -8.2974e+02],\n",
      "        [ 0.0000e+00, -5.9805e+02],\n",
      "        [ 0.0000e+00, -2.3546e+02]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00040 | Loss 88.9855 | Time(s) 0.3108\n",
      "tensor([[-2.3126e-02, -3.7783e+00],\n",
      "        [-6.5083e-02, -2.7645e+00],\n",
      "        [-8.1698e-03, -4.8114e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -7.6447e+02],\n",
      "        [ 0.0000e+00, -5.5385e+02],\n",
      "        [ 0.0000e+00, -1.8969e+02]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00041 | Loss 80.5941 | Time(s) 0.3109\n",
      "tensor([[-2.2949e+00, -1.0622e-01],\n",
      "        [-3.4472e+00, -3.2353e-02],\n",
      "        [-1.1981e-02, -4.4304e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -6.6593e+02],\n",
      "        [ 0.0000e+00, -4.8689e+02],\n",
      "        [ 0.0000e+00, -1.1722e+02]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00042 | Loss 73.6474 | Time(s) 0.3113\n",
      "tensor([[-9.1074e+00, -1.1086e-04],\n",
      "        [-1.0531e+01, -2.6703e-05],\n",
      "        [-1.8648e-02, -3.9913e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -5.5223e+02],\n",
      "        [ 0.0000e+00, -4.0950e+02],\n",
      "        [ 0.0000e+00, -3.1739e+01]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00043 | Loss 69.8088 | Time(s) 0.3113\n",
      "tensor([[-1.6352e+01, -1.1921e-07],\n",
      "        [-1.7982e+01,  0.0000e+00],\n",
      "        [-2.9531e-02, -3.5370e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -4.3322e+02],\n",
      "        [ 0.0000e+00, -3.2852e+02],\n",
      "        [-5.7818e+01,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00044 | Loss 67.4538 | Time(s) 0.3111\n",
      "tensor([[-2.3390e+01,  0.0000e+00],\n",
      "        [-2.5218e+01,  0.0000e+00],\n",
      "        [-4.5953e-02, -3.1030e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -3.1710e+02],\n",
      "        [ 0.0000e+00, -2.4953e+02],\n",
      "        [-1.4396e+02,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00045 | Loss 65.8423 | Time(s) 0.3113\n",
      "tensor([[-3.0074e+01,  0.0000e+00],\n",
      "        [-3.2090e+01,  0.0000e+00],\n",
      "        [-6.9593e-02, -2.6997e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -2.0554e+02],\n",
      "        [ 0.0000e+00, -1.7380e+02],\n",
      "        [-2.2479e+02,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00046 | Loss 65.4392 | Time(s) 0.3115\n",
      "tensor([[-3.6290e+01,  0.0000e+00],\n",
      "        [-3.8479e+01,  0.0000e+00],\n",
      "        [-1.0165e-01, -2.3366e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.0056e+02],\n",
      "        [ 0.0000e+00, -1.0270e+02],\n",
      "        [-2.9880e+02,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00047 | Loss 66.3050 | Time(s) 0.3115\n",
      "tensor([[-4.1763e+01,  0.0000e+00],\n",
      "        [-4.4101e+01,  0.0000e+00],\n",
      "        [-1.4059e-01, -2.0314e+00],\n",
      "        ...,\n",
      "        [-8.1423e-04, -7.1136e+00],\n",
      "        [ 0.0000e+00, -3.9554e+01],\n",
      "        [-3.6251e+02,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00048 | Loss 68.0041 | Time(s) 0.3116\n",
      "tensor([[-4.6153e+01,  0.0000e+00],\n",
      "        [-4.8609e+01,  0.0000e+00],\n",
      "        [-1.8055e-01, -1.8007e+00],\n",
      "        ...,\n",
      "        [-6.8791e+01,  0.0000e+00],\n",
      "        [-1.1611e+01, -9.0599e-06],\n",
      "        [-4.1159e+02,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n",
      "Epoch 00049 | Loss 69.7335 | Time(s) 0.3118\n",
      "tensor([[-4.9099e+01,  0.0000e+00],\n",
      "        [-5.1628e+01,  0.0000e+00],\n",
      "        [-2.0988e-01, -1.6643e+00],\n",
      "        ...,\n",
      "        [-1.1976e+02,  0.0000e+00],\n",
      "        [-4.5531e+01,  0.0000e+00],\n",
      "        [-4.4081e+02,  0.0000e+00]], grad_fn=<LogSoftmaxBackward>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-5)\n",
    "dur = []\n",
    "for epoch in range(50):\n",
    "    #if epoch >= 3:\n",
    "    t0 = time.time()\n",
    "\n",
    "    logits = net(graph_train, train_feature)\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp, train_label)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #if epoch >=3:\n",
    "    dur.append(time.time() - t0)\n",
    "\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), np.mean(dur)))\n",
    "    print(logp,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#玩具数据集用以检验图结构生成的准确率。\n",
    "#测试发现difflib的相似度量是字符级的\n",
    "'''\n",
    "dataframe = pd.DataFrame([\n",
    "        [\"a\", \"111222\", \"computer Science\", \"bruce\"],\n",
    "        [\"b\", \"111222\", \"computer Science\", \"Bruce Lee\"],\n",
    "        [\"c\", \"111222\", \"computer Science\", \"mike ,john\"],\n",
    "        [\"a\", \"111223\", \"Hassdsdsaad\", \"kkl\"],\n",
    "        [\"d\", \"111223\", \"Hassdsdaaad\", \"kkkl\"],\n",
    "        [\"c\", \"111224\", \"asdfgh\", \"zxcr\"]\n",
    "    ],\n",
    "    columns=[\"source\", \"isbn\", \"name\", \"author\"]\n",
    ")\n",
    "g = generate_DGLGraph(dataframe)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_gpu]",
   "language": "python",
   "name": "conda-env-pytorch_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
