{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import difflib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl import DGLGraph\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#来自DGLGraph tutorial，in_feats、out_feats需要根据feature长度和分类数改动改动\n",
    "class NodeApplyModule(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        #self.conv = nn.Conv1d(in_feats,out_feats,1)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, node):\n",
    "        #h = self.conv(node.data['h'])\n",
    "        h = self.linear(node.data['h'])\n",
    "        if self.activation is not None:\n",
    "            h = self.activation(h,inplace=True)\n",
    "        return {'h' : h}\n",
    "\n",
    "gcn_msg = fn.copy_src(src='h', out='m')\n",
    "gcn_reduce = fn.sum(msg='m', out='h')\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(gcn_msg, gcn_reduce)\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #self.gcn1 = GCN(605, 384, F.relu)\n",
    "        #self.gcn1 = GCN(2304, 384, F.relu)\n",
    "        #self.gcn1 = GCN(768, 384, F.relu)\n",
    "        #self.gcn2 = GCN(384, 192, F.relu)\n",
    "        #self.fc = nn.Linear(192, 2)\n",
    "        #self.fc1 = nn.Linear(605,288)\n",
    "        self.fc1_1 = nn.Linear(768,96)\n",
    "        self.fc1_2 = nn.Linear(768,96)\n",
    "        self.fc1_3 = nn.Linear(768,96)\n",
    "        self.gcn2 = GCN(288,96,F.relu)\n",
    "        #self.gcn3 = GCN(96,96,F.relu)\n",
    "        self.fc4 = nn.Linear(96,2)\n",
    "        \n",
    "    def forward(self, g, features):\n",
    "        #x = self.fc1(features)\n",
    "        x1 = self.fc1_1(features[:,0:768])\n",
    "        x2 = self.fc1_2(features[:,768:-768])\n",
    "        x3 = self.fc1_3(features[:,-768:])\n",
    "        x = torch.cat((x1,x2,x3),1)\n",
    "        x = self.gcn2(g,x)\n",
    "        #x = self.gcn3(g,x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, pred_prob):\n",
    "        #self.eval()\n",
    "        pred = F.softmax(pred_prob)\n",
    "        ans = []\n",
    "        for t in pred:\n",
    "            if t[0]>t[1]:\n",
    "                ans.append(0)\n",
    "            else:\n",
    "                ans.append(1)\n",
    "        return torch.tensor(ans)\n",
    "    '''\n",
    "    def predict(self, g, features):\n",
    "        #self.eval()\n",
    "        pred = F.softmax(self.forward(g, features))\n",
    "        ans = []\n",
    "        for t in pred:\n",
    "            if t[0]>t[1]:\n",
    "                ans.append(0)\n",
    "            else:\n",
    "                ans.append(1)\n",
    "        return torch.tensor(ans)\n",
    "    '''\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读取DealWithData中golden数据\n",
    "data_golden = pd.read_csv( './DataSet/book/golden/claims_golden2.txt' , sep='\\t' )\n",
    "data_golden['encode'] = torch.load('./DataSet/book/golden/claims_golden_encode_v2.pt')\n",
    "#data_golden['encode'] = torch.load('./DataSet/book/golden/claims_golden_encode.pt')\n",
    "GoldenLabel = pd.read_table(\"./DataSet/book/book_golden.txt\" , sep='\\t' , header=None , names=['isbn','author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#使用遮盖而非分隔的方式划分数据集\n",
    "def divide_dataset(dfw,dfk,test_ratio):\n",
    "    a = np.random.choice(len(dfk), int(len(dfk)*test_ratio), replace=False)\n",
    "    test_set = set()\n",
    "    for i in range(0,a.shape[0]):\n",
    "        test_set.add(dfk.loc[a[i]]['isbn'])\n",
    "    train_mask = torch.Tensor(size=[len(dfw)]).bool()\n",
    "    test_mask = torch.Tensor(size=[len(dfw)]).bool()\n",
    "    for i in range(0,len(dfw)):\n",
    "        if dfw.loc[i]['isbn'] in test_set:\n",
    "            test_mask[i] = True\n",
    "            train_mask[i] = False\n",
    "        else:\n",
    "            test_mask[i] = False\n",
    "            train_mask[i] = True\n",
    "    return train_mask,test_mask\n",
    "train_mask,test_mask = divide_dataset(data_golden,GoldenLabel,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#构建图结构函数，根据训练集和测试集连边 -> 改为基于全体数据连边\n",
    "#此处可优化复杂度？n**2/2 -> kn^?\n",
    "#此处可根据'book_name'相似度进一步加边\n",
    "#graph_whole3为添加自环的版本 124468 -> 126807\n",
    "\n",
    "#使用Jaccard相似度之后边数 124446 -> 124468 ，影响不大？\n",
    "\n",
    "def sim_Jaccard (str1,str2) :\n",
    "    set1 = set( str1.lower().replace(';',' ').replace(',',' ').replace('.',' ').replace(':',' ').replace('&',' ').\n",
    "               replace('/',' ').replace('\\'',' ').replace('(author)',' ').replace('(joint author)',' ').split() )\n",
    "    set2 = set( str2.lower().replace(';',' ').replace(',',' ').replace('.',' ').replace(':',' ').replace('&',' ').\n",
    "               replace('/',' ').replace('\\'',' ').replace('(author)',' ').replace('(joint author)',' ').split() )\n",
    "    return len(set1&set2)/len(set1|set2)\n",
    "\n",
    "def generate_DGLGraph(df):\n",
    "    g = DGLGraph()\n",
    "    g.add_nodes(df.shape[0])\n",
    "    \n",
    "    source_list = df['source'].drop_duplicates().reset_index(drop=True)\n",
    "    isbn_list = df['isbn'].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    for index,row in df.iterrows():\n",
    "        g.add_edge(index,index)\n",
    "    \n",
    "    for index,value in source_list.iteritems():\n",
    "        df_slice = df[df['source']==value]\n",
    "    #for k in range(0,1):\n",
    "    #    df_slice = df[df['source']==source_list.loc[k]]\n",
    "        for i in range(0,len(df_slice)):\n",
    "            for j in range(i+1,len(df_slice)):\n",
    "                g.add_edge(df_slice.iloc[i].name,df_slice.iloc[j].name)\n",
    "                g.add_edge(df_slice.iloc[j].name,df_slice.iloc[i].name)\n",
    "\n",
    "    for index,value in isbn_list.iteritems():\n",
    "        df_slice = df[df['isbn']==value]\n",
    "    #for k in range(0,1):\n",
    "    #    df_slice = df[df['isbn']==isbn_list.loc[k]]\n",
    "        for i in range(0,len(df_slice)):\n",
    "            for j in range(i+1,len(df_slice)):\n",
    "                if( sim_Jaccard( df_slice.iloc[i]['author'] , df_slice.iloc[j]['author'] ) >= 0.8 ):\n",
    "                    g.add_edge(df_slice.iloc[i].name,df_slice.iloc[j].name)\n",
    "                    g.add_edge(df_slice.iloc[j].name,df_slice.iloc[i].name)\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "graph_whole = generate_DGLGraph(data_golden)\n",
    "#graph_train = generate_DGLGraph(data_train)\n",
    "#graph_test = generate_DGLGraph(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "构建图结构函数，该版本弃置\n",
    "def generate_DGLGraph(df):\n",
    "    #edge_norm = []\n",
    "    g = DGLGraph()\n",
    "    g.add_nodes(df.shape[0])\n",
    "    for i in range(0,len(df)):\n",
    "        g.add_edge(i,i)\n",
    "    for i in range(0,len(df)):\n",
    "        for j in range(i+1,len(df)):\n",
    "            if (df.loc[i][\"source\"]==df.loc[j][\"source\"]):\n",
    "                g.add_edge(i,j)\n",
    "                #edge_norm.append(1.0)\n",
    "                g.add_edge(j,i)\n",
    "                #edge_norm.append(1.0)\n",
    "            elif (df.loc[i][\"isbn\"]==df.loc[j][\"isbn\"]):\n",
    "                str1 = df.loc[i]['author']\n",
    "                str2 = df.loc[j]['author']\n",
    "                #print(str1,str2,difflib.SequenceMatcher(None,str1,str2).quick_ratio())\n",
    "                #if ( difflib.SequenceMatcher(None,str1,str2).quick_ratio()>0.8 ):\n",
    "                if ( sim_Jaccard(str1,str2)>0.8 ):\n",
    "                    g.add_edge(i,j)\n",
    "                    #edge_norm.append(1.0)\n",
    "                    g.add_edge(j,i)\n",
    "                    #edge_norm.append(1.0)\n",
    "    #edge_norm = torch.Tensor(edge_norm).unsqueeze(1)\n",
    "    #g.edata.update({ 'norm': edge_norm })\n",
    "    return g\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#存储图结构\n",
    "file = open('./DataSet/book/golden/graph_whole3.pickle', 'wb')\n",
    "pickle.dump(graph_whole, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加载图结构\n",
    "with open('./DataSet/book/golden/graph_whole3.pickle', 'rb') as file:\n",
    "    graph_whole =pickle.load(file)\n",
    "#with open('./DataSet/book/golden/graph_train.pickle', 'rb') as file:\n",
    "#    graph_train =pickle.load(file)\n",
    "#with open('./DataSet/book/golden/graph_test.pickle', 'rb') as file:\n",
    "#    graph_test =pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#存储和加载图结构的失败尝试\n",
    "'''\n",
    "graph_train_netx = graph_train.to_networkx()\n",
    "graph_test_netx = graph_test.to_networkx()\n",
    "nx.write_gexf(graph_train_netx,'./DataSet/book/golden/train_graph.gexf')\n",
    "nx.write_gexf(graph_test_netx,'./DataSet/book/golden/test_graph.gexf')\n",
    "\n",
    "graph_train_netx = nx.read_gexf('./DataSet/book/golden/train_graph.gexf')\n",
    "graph_test_netx = nx.read_gexf('./DataSet/book/golden/test_graph.gexf')\n",
    "\n",
    "graph_train2 = DGLGraph(graph_train_netx)\n",
    "#graph_train2.from_networkx(graph_train_netx)\n",
    "graph_test2 = DGLGraph(graph_test_netx)\n",
    "#graph_test2.from_networkx(graph_test_netx)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_featureNlabel(df):\n",
    "    a = torch.zeros(1,len(df.loc[0]['encode']))\n",
    "    c = torch.zeros(1)\n",
    "    for _,row in df.iterrows():\n",
    "        b = row['encode'].reshape([1,-1])\n",
    "        a = torch.cat((a,b),0)\n",
    "        if(row['label']):\n",
    "            d = torch.ones(1)\n",
    "        else:\n",
    "            d = torch.zeros(1)\n",
    "        c = torch.cat((c,d),-1)\n",
    "    return a[1:,:],c[1:].long()\n",
    "#_,whole_label = extract_featureNlabel(data_golden)\n",
    "#whole_feature = torch.load('./DataSet/book/golden/claims_golden_encode_tfidf.pt')\n",
    "whole_feature,whole_label = extract_featureNlabel(data_golden)\n",
    "#train_feature,train_label = extract_featureNlabel(data_train)\n",
    "#test_feature,test_label = extract_featureNlabel(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果比对\n",
    "\n",
    "a:1\\*768(bert)\n",
    "\n",
    "b:1\\*3\\*768(bert_v2)\n",
    "\n",
    "c:1\\*605(tfidf)\n",
    "\n",
    "train:test=1:1\n",
    "\n",
    "| method | len | lr | epoch | loss | accu |\n",
    "|:----: | :----: | :----: | :----: | :----: | :----: |\n",
    "| a | 768 | 1e-5 | 200 | 1.xx | 0.6 | \n",
    "| b | 2304 | 1e-5 | 200 | 5.xx | 0.7 |\n",
    "| c | 605 | 1e-5 | 200 | 0.5x | 0.7 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\acaconda\\envs\\pytorch_gpu\\lib\\site-packages\\ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 0.9127 | Time(s) 0.5376 | Train_Accu 0.673885 | Test_Accu 0.742837\n",
      "Epoch 00001 | Loss 1.4365 | Time(s) 0.5191 | Train_Accu 0.653503 | Test_Accu 0.654011\n",
      "Epoch 00002 | Loss 0.9632 | Time(s) 0.5196 | Train_Accu 0.676433 | Test_Accu 0.739255\n",
      "Epoch 00003 | Loss 0.8208 | Time(s) 0.5146 | Train_Accu 0.692994 | Test_Accu 0.684814\n",
      "Epoch 00004 | Loss 0.7858 | Time(s) 0.5118 | Train_Accu 0.694268 | Test_Accu 0.713467\n",
      "Epoch 00005 | Loss 0.7928 | Time(s) 0.5105 | Train_Accu 0.719745 | Test_Accu 0.748567\n",
      "Epoch 00006 | Loss 0.8344 | Time(s) 0.5101 | Train_Accu 0.704459 | Test_Accu 0.680516\n",
      "Epoch 00007 | Loss 0.8699 | Time(s) 0.5099 | Train_Accu 0.695541 | Test_Accu 0.738539\n",
      "Epoch 00008 | Loss 1.0205 | Time(s) 0.5096 | Train_Accu 0.698089 | Test_Accu 0.670487\n",
      "Epoch 00009 | Loss 0.7886 | Time(s) 0.5086 | Train_Accu 0.712102 | Test_Accu 0.738539\n",
      "Epoch 00010 | Loss 0.6978 | Time(s) 0.5097 | Train_Accu 0.732484 | Test_Accu 0.699140\n",
      "Epoch 00011 | Loss 0.6121 | Time(s) 0.5092 | Train_Accu 0.757962 | Test_Accu 0.742837\n",
      "Epoch 00012 | Loss 0.5721 | Time(s) 0.5089 | Train_Accu 0.757962 | Test_Accu 0.729226\n",
      "Epoch 00013 | Loss 0.5665 | Time(s) 0.5087 | Train_Accu 0.759236 | Test_Accu 0.729226\n",
      "Epoch 00014 | Loss 0.5870 | Time(s) 0.5090 | Train_Accu 0.747771 | Test_Accu 0.744269\n",
      "Epoch 00015 | Loss 0.7390 | Time(s) 0.5088 | Train_Accu 0.717197 | Test_Accu 0.672636\n",
      "Epoch 00016 | Loss 0.8739 | Time(s) 0.5119 | Train_Accu 0.695541 | Test_Accu 0.691261\n",
      "Epoch 00017 | Loss 1.0694 | Time(s) 0.5113 | Train_Accu 0.718471 | Test_Accu 0.671203\n",
      "Epoch 00018 | Loss 0.5088 | Time(s) 0.5109 | Train_Accu 0.773248 | Test_Accu 0.736390\n",
      "Epoch 00019 | Loss 0.5298 | Time(s) 0.5102 | Train_Accu 0.764331 | Test_Accu 0.746418\n",
      "Epoch 00020 | Loss 0.9266 | Time(s) 0.5109 | Train_Accu 0.727389 | Test_Accu 0.676218\n",
      "Epoch 00021 | Loss 0.6592 | Time(s) 0.5109 | Train_Accu 0.742675 | Test_Accu 0.713467\n",
      "Epoch 00022 | Loss 0.4829 | Time(s) 0.5114 | Train_Accu 0.797452 | Test_Accu 0.733524\n",
      "Epoch 00023 | Loss 0.4512 | Time(s) 0.5112 | Train_Accu 0.811465 | Test_Accu 0.750716\n",
      "Epoch 00024 | Loss 0.5582 | Time(s) 0.5114 | Train_Accu 0.778344 | Test_Accu 0.725645\n",
      "Epoch 00025 | Loss 0.7065 | Time(s) 0.5111 | Train_Accu 0.745223 | Test_Accu 0.684814\n",
      "Epoch 00026 | Loss 0.5308 | Time(s) 0.5110 | Train_Accu 0.780892 | Test_Accu 0.725645\n",
      "Epoch 00027 | Loss 0.4358 | Time(s) 0.5110 | Train_Accu 0.814013 | Test_Accu 0.749284\n",
      "Epoch 00028 | Loss 0.3877 | Time(s) 0.5107 | Train_Accu 0.821656 | Test_Accu 0.744986\n",
      "Epoch 00029 | Loss 0.4177 | Time(s) 0.5107 | Train_Accu 0.810191 | Test_Accu 0.742120\n",
      "Epoch 00030 | Loss 0.5537 | Time(s) 0.5113 | Train_Accu 0.765605 | Test_Accu 0.693410\n",
      "Epoch 00031 | Loss 0.6405 | Time(s) 0.5119 | Train_Accu 0.757962 | Test_Accu 0.689828\n",
      "Epoch 00032 | Loss 0.7692 | Time(s) 0.5123 | Train_Accu 0.735032 | Test_Accu 0.681948\n",
      "Epoch 00033 | Loss 0.4732 | Time(s) 0.5121 | Train_Accu 0.788535 | Test_Accu 0.716332\n",
      "Epoch 00034 | Loss 0.3779 | Time(s) 0.5119 | Train_Accu 0.830573 | Test_Accu 0.762894\n",
      "Epoch 00035 | Loss 0.3926 | Time(s) 0.5130 | Train_Accu 0.830573 | Test_Accu 0.750000\n",
      "Epoch 00036 | Loss 0.4631 | Time(s) 0.5128 | Train_Accu 0.798726 | Test_Accu 0.722779\n",
      "Epoch 00037 | Loss 0.6026 | Time(s) 0.5129 | Train_Accu 0.754140 | Test_Accu 0.696991\n",
      "Epoch 00038 | Loss 0.5283 | Time(s) 0.5126 | Train_Accu 0.777070 | Test_Accu 0.699857\n",
      "Epoch 00039 | Loss 0.4912 | Time(s) 0.5130 | Train_Accu 0.800000 | Test_Accu 0.719914\n",
      "Epoch 00040 | Loss 0.3983 | Time(s) 0.5128 | Train_Accu 0.826752 | Test_Accu 0.735673\n",
      "Epoch 00041 | Loss 0.3536 | Time(s) 0.5126 | Train_Accu 0.839490 | Test_Accu 0.755731\n",
      "Epoch 00042 | Loss 0.3506 | Time(s) 0.5125 | Train_Accu 0.845860 | Test_Accu 0.760029\n",
      "Epoch 00043 | Loss 0.3832 | Time(s) 0.5127 | Train_Accu 0.833121 | Test_Accu 0.737822\n",
      "Epoch 00044 | Loss 0.4708 | Time(s) 0.5126 | Train_Accu 0.801274 | Test_Accu 0.722779\n",
      "Epoch 00045 | Loss 0.5571 | Time(s) 0.5127 | Train_Accu 0.778344 | Test_Accu 0.682665\n",
      "Epoch 00046 | Loss 0.6897 | Time(s) 0.5128 | Train_Accu 0.743949 | Test_Accu 0.686246\n",
      "Epoch 00047 | Loss 0.4894 | Time(s) 0.5127 | Train_Accu 0.798726 | Test_Accu 0.698424\n",
      "Epoch 00048 | Loss 0.3895 | Time(s) 0.5127 | Train_Accu 0.834395 | Test_Accu 0.747135\n",
      "Epoch 00049 | Loss 0.3380 | Time(s) 0.5131 | Train_Accu 0.848408 | Test_Accu 0.755731\n",
      "Epoch 00050 | Loss 0.3612 | Time(s) 0.5135 | Train_Accu 0.844586 | Test_Accu 0.744269\n",
      "Epoch 00051 | Loss 0.4393 | Time(s) 0.5134 | Train_Accu 0.819108 | Test_Accu 0.733524\n",
      "Epoch 00052 | Loss 0.4823 | Time(s) 0.5132 | Train_Accu 0.797452 | Test_Accu 0.691977\n",
      "Epoch 00053 | Loss 0.5538 | Time(s) 0.5130 | Train_Accu 0.765605 | Test_Accu 0.702722\n",
      "Epoch 00054 | Loss 0.4678 | Time(s) 0.5130 | Train_Accu 0.798726 | Test_Accu 0.696991\n",
      "Epoch 00055 | Loss 0.4198 | Time(s) 0.5127 | Train_Accu 0.833121 | Test_Accu 0.741404\n",
      "Epoch 00056 | Loss 0.3473 | Time(s) 0.5127 | Train_Accu 0.850955 | Test_Accu 0.744986\n",
      "Epoch 00057 | Loss 0.3267 | Time(s) 0.5125 | Train_Accu 0.852229 | Test_Accu 0.753582\n",
      "Epoch 00058 | Loss 0.3495 | Time(s) 0.5124 | Train_Accu 0.852229 | Test_Accu 0.760745\n",
      "Epoch 00059 | Loss 0.3832 | Time(s) 0.5124 | Train_Accu 0.836943 | Test_Accu 0.717765\n",
      "Epoch 00060 | Loss 0.4632 | Time(s) 0.5124 | Train_Accu 0.775796 | Test_Accu 0.709885\n",
      "Epoch 00061 | Loss 0.5202 | Time(s) 0.5125 | Train_Accu 0.783439 | Test_Accu 0.681948\n",
      "Epoch 00062 | Loss 0.6302 | Time(s) 0.5126 | Train_Accu 0.745223 | Test_Accu 0.686963\n",
      "Epoch 00063 | Loss 0.4959 | Time(s) 0.5126 | Train_Accu 0.791083 | Test_Accu 0.684814\n",
      "Epoch 00064 | Loss 0.4298 | Time(s) 0.5125 | Train_Accu 0.812739 | Test_Accu 0.732092\n",
      "Epoch 00065 | Loss 0.3471 | Time(s) 0.5124 | Train_Accu 0.859873 | Test_Accu 0.734241\n",
      "Epoch 00066 | Loss 0.3211 | Time(s) 0.5131 | Train_Accu 0.859873 | Test_Accu 0.752865\n",
      "Epoch 00067 | Loss 0.3420 | Time(s) 0.5131 | Train_Accu 0.853503 | Test_Accu 0.760029\n",
      "Epoch 00068 | Loss 0.3691 | Time(s) 0.5130 | Train_Accu 0.847134 | Test_Accu 0.728510\n",
      "Epoch 00069 | Loss 0.4111 | Time(s) 0.5130 | Train_Accu 0.824204 | Test_Accu 0.734241\n",
      "Epoch 00070 | Loss 0.4311 | Time(s) 0.5130 | Train_Accu 0.810191 | Test_Accu 0.695559\n",
      "Epoch 00071 | Loss 0.5105 | Time(s) 0.5130 | Train_Accu 0.761783 | Test_Accu 0.692693\n",
      "Epoch 00072 | Loss 0.5156 | Time(s) 0.5129 | Train_Accu 0.788535 | Test_Accu 0.678367\n",
      "Epoch 00073 | Loss 0.5537 | Time(s) 0.5128 | Train_Accu 0.749045 | Test_Accu 0.694126\n",
      "Epoch 00074 | Loss 0.4305 | Time(s) 0.5128 | Train_Accu 0.817834 | Test_Accu 0.704871\n",
      "Epoch 00075 | Loss 0.3638 | Time(s) 0.5127 | Train_Accu 0.848408 | Test_Accu 0.753582\n",
      "Epoch 00076 | Loss 0.3181 | Time(s) 0.5126 | Train_Accu 0.862420 | Test_Accu 0.752865\n",
      "Epoch 00077 | Loss 0.3552 | Time(s) 0.5125 | Train_Accu 0.856051 | Test_Accu 0.734241\n",
      "Epoch 00078 | Loss 0.4403 | Time(s) 0.5125 | Train_Accu 0.814013 | Test_Accu 0.729226\n",
      "Epoch 00079 | Loss 0.4399 | Time(s) 0.5124 | Train_Accu 0.806369 | Test_Accu 0.691261\n",
      "Epoch 00080 | Loss 0.4552 | Time(s) 0.5125 | Train_Accu 0.800000 | Test_Accu 0.727077\n",
      "Epoch 00081 | Loss 0.3935 | Time(s) 0.5124 | Train_Accu 0.829299 | Test_Accu 0.710602\n",
      "Epoch 00082 | Loss 0.3651 | Time(s) 0.5123 | Train_Accu 0.849682 | Test_Accu 0.760029\n",
      "Epoch 00083 | Loss 0.3248 | Time(s) 0.5124 | Train_Accu 0.859873 | Test_Accu 0.734957\n",
      "Epoch 00084 | Loss 0.3046 | Time(s) 0.5124 | Train_Accu 0.863694 | Test_Accu 0.756447\n",
      "Epoch 00085 | Loss 0.2976 | Time(s) 0.5123 | Train_Accu 0.859873 | Test_Accu 0.752865\n",
      "Epoch 00086 | Loss 0.3032 | Time(s) 0.5124 | Train_Accu 0.867516 | Test_Accu 0.739255\n",
      "Epoch 00087 | Loss 0.3300 | Time(s) 0.5123 | Train_Accu 0.861146 | Test_Accu 0.760745\n",
      "Epoch 00088 | Loss 0.4113 | Time(s) 0.5125 | Train_Accu 0.822930 | Test_Accu 0.693410\n",
      "Epoch 00089 | Loss 0.6394 | Time(s) 0.5123 | Train_Accu 0.746497 | Test_Accu 0.684814\n",
      "Epoch 00090 | Loss 0.6716 | Time(s) 0.5124 | Train_Accu 0.760510 | Test_Accu 0.651862\n",
      "Epoch 00091 | Loss 0.6344 | Time(s) 0.5122 | Train_Accu 0.746497 | Test_Accu 0.687679\n",
      "Epoch 00092 | Loss 0.3775 | Time(s) 0.5123 | Train_Accu 0.843312 | Test_Accu 0.716332\n",
      "Epoch 00093 | Loss 0.3104 | Time(s) 0.5122 | Train_Accu 0.870064 | Test_Accu 0.752865\n",
      "Epoch 00094 | Loss 0.3524 | Time(s) 0.5121 | Train_Accu 0.853503 | Test_Accu 0.741404\n",
      "Epoch 00095 | Loss 0.3926 | Time(s) 0.5120 | Train_Accu 0.824204 | Test_Accu 0.715616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00096 | Loss 0.4126 | Time(s) 0.5120 | Train_Accu 0.836943 | Test_Accu 0.740688\n",
      "Epoch 00097 | Loss 0.3411 | Time(s) 0.5119 | Train_Accu 0.858599 | Test_Accu 0.729943\n",
      "Epoch 00098 | Loss 0.3012 | Time(s) 0.5120 | Train_Accu 0.867516 | Test_Accu 0.744986\n",
      "Epoch 00099 | Loss 0.2968 | Time(s) 0.5119 | Train_Accu 0.867516 | Test_Accu 0.749284\n"
     ]
    }
   ],
   "source": [
    "dur = []\n",
    "for epoch in range(100):\n",
    "    \n",
    "    t0 = time.time()\n",
    "\n",
    "    pred_prob = net.forward(graph_whole, whole_feature)\n",
    "    loss = criterion(pred_prob[train_mask],whole_label[train_mask])\n",
    "    \n",
    "    #pred_prob = net.forward(graph_train, train_feature)\n",
    "    #loss = criterion(pred_prob,train_label)\n",
    "    #pred_label = net.predict(graph_whole, whole_feature)\n",
    "    pred_label = net.predict(pred_prob)\n",
    "    train_accu = accuracy_score(pred_label[train_mask],whole_label[train_mask])\n",
    "    test_accu = accuracy_score(pred_label[test_mask],whole_label[test_mask])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    dur.append(time.time() - t0)\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Time(s) {:.4f} | Train_Accu {:4f} | Test_Accu {:4f}\".format(\n",
    "        epoch, loss.item(), np.mean(dur), train_accu,test_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#玩具数据集用以检验图结构生成的准确率。\n",
    "#测试发现difflib的相似度量是字符级的\n",
    "'''\n",
    "dataframe = pd.DataFrame([\n",
    "        [\"a\", \"111222\", \"computer Science\", \"bruce\"],\n",
    "        [\"b\", \"111222\", \"computer Science\", \"Bruce Lee\"],\n",
    "        [\"c\", \"111222\", \"computer Science\", \"mike ,john\"],\n",
    "        [\"a\", \"111223\", \"Hassdsdsaad\", \"kkl\"],\n",
    "        [\"d\", \"111223\", \"Hassdsdaaad\", \"kkkl\"],\n",
    "        [\"c\", \"111224\", \"asdfgh\", \"zxcr\"]\n",
    "    ],\n",
    "    columns=[\"source\", \"isbn\", \"name\", \"author\"]\n",
    ")\n",
    "g = generate_DGLGraph(dataframe)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\acaconda\\envs\\pytorch_gpu\\lib\\site-packages\\ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meadors, Todd; Schmidt, Cheryl Ann vs meadors, todd ;  schmidt, cheryl a.;   0.6666666666666666\n",
      "C Bala Kumar, Paul Kline, Tim Thompson vs kumar, c bala;  kline, paul j.;  thompson, tim j.;   0.875\n",
      "By (author) Hoos, Holger H. By (author) St&uuml;tzle, Thomas vs hoos, holger h.;   stutzle, thomas;   0.4444444444444444\n",
      "Loshin, Peter vs loshin, pete ;   0.3333333333333333\n",
      "By (author) Harmon, Paul vs harmon, paul ;   0.6666666666666666\n",
      "Edited by Jones, Karen Sparck Edited by Willett, Peter vs jones, karen sparck;  willett, peter ;   0.7142857142857143\n",
      "Fortier, Paul J.; Michel, Howard vs fortier, paul j.;  michel, howard e.;   0.8333333333333334\n",
      "Guy Steele vs steele, guy l.;   0.6666666666666666\n",
      "C. B. Jenssen, T. Kvamdal, H. I. Andersson vs jenssen, c. b.;  kvamdal, t. ;  andersson, h. i.;  Ecer, A.;  Periaux, J.;  Satofuka, N.;  Fox, P.; 0.5\n",
      "George F. Coulouris , Jean Dollimore , Tim Kindberg vs coulouris, george ;  dollimore, jean ;  kindberg, tim ;   0.8571428571428571\n",
      "Nick Rozanski, E&oacute;in Woods vs rozanski, nick ;  woods, eoin ;   0.42857142857142855\n",
      "W. Richard Stevens, Gary Wright vs stevens, w. richard;  wright, gary r. ;   0.8333333333333334\n",
      "MUSSER, DAVID R. - GILLMER J. DERGE - ATUL SAINI vs musser, david r.;  derge, gillmer j.;  saini, atul ;   0.8888888888888888\n",
      "Hornick, Mark F. Marcade, Erik vs hornick, mark f.;  marcade, erik ;  venkayala, sunil ;   0.7142857142857143\n",
      "(0.7666666666666667, 0.9237103174603175)\n"
     ]
    }
   ],
   "source": [
    "def add_confidence(df,prob,col_name='fact_confidence'):\n",
    "    df[col_name] = None\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i,col_name] = float(prob[i][1])\n",
    "    return df\n",
    "\n",
    "def sim_Jaccard (str1,str2) :\n",
    "    set1 = set( str1.lower().replace(';',' ').replace(',',' ').replace('.',' ').replace(':',' ').replace('&',' ').\n",
    "               replace('/',' ').replace('\\'',' ').replace('(author)',' ').replace('(joint author)',' ').split() )\n",
    "    set2 = set( str2.lower().replace(';',' ').replace(',',' ').replace('.',' ').replace(':',' ').replace('&',' ').\n",
    "               replace('/',' ').replace('\\'',' ').replace('(author)',' ').replace('(joint author)',' ').split() )\n",
    "    return len(set1&set2)/len(set1|set2)\n",
    "\n",
    "def MV(df,indexK='isbn',answer='author',withWeight=False,weight='confidence'):\n",
    "    df_mv = pd.DataFrame(columns=[indexK,answer])\n",
    "    for indexV in df[indexK].unique():\n",
    "        data_slice = df[df[indexK]==indexV]\n",
    "        vote_dict = {}\n",
    "        for index,row in data_slice.iterrows():\n",
    "            flag = False\n",
    "            for key in vote_dict.keys():\n",
    "                if ( sim_Jaccard(key,row[answer])>=0.8 ):\n",
    "                    flag = True\n",
    "                    if(not withWeight):\n",
    "                        vote_dict[key] += 1\n",
    "                    else:\n",
    "                        vote_dict[key] += float(row[weight])\n",
    "                    break\n",
    "            if (not flag):\n",
    "                if(not withWeight):\n",
    "                    vote_dict[row[answer]] = 1\n",
    "                else:\n",
    "                    vote_dict[row[answer]] = float(row[weight])\n",
    "        vote_list = sorted(vote_dict.items(), key=lambda d:d[1],reverse=True)\n",
    "        #print({indexK:indexV,answer:vote_list[0][0]})\n",
    "        df_mv = df_mv.append({indexK:indexV,answer:vote_list[0][0]},ignore_index=True)\n",
    "    return df_mv\n",
    "\n",
    "def JudgeAccu(label,pred,pred_col='author'):\n",
    "    score1 = 0\n",
    "    score2 = 0\n",
    "    for index,row in pred.iterrows():\n",
    "        if not(index in label.index):\n",
    "            print(index,'no answer')\n",
    "            score1 += 0 \n",
    "            score1 += 0\n",
    "        elif sim_Jaccard(row[pred_col],label.loc[index][pred_col])>=0.9:\n",
    "            score1 +=1\n",
    "            score2 +=1\n",
    "        else:\n",
    "            print(row[pred_col],\"vs\",label.loc[index][pred_col],sim_Jaccard(row[pred_col],label.loc[index][pred_col]))\n",
    "            score1 += 0\n",
    "            score2 += sim_Jaccard(row[pred_col],label.loc[index][pred_col])\n",
    "    return score1/len(pred),score2/len(pred)\n",
    "\n",
    "data_withConfidence = add_confidence(data_golden,F.softmax(pred_prob))\n",
    "\n",
    "df_mv = MV(data_withConfidence[test_mask.numpy()],withWeight=True,weight='fact_confidence')\n",
    "df_mv.to_csv( './DataSet/book/golden/GCNResult.txt' , sep='\\t' , index=False )\n",
    "\n",
    "label = pd.read_csv('./DataSet/book/book_golden.txt',sep='\\t',low_memory=False,names=['isbn','author'],header=None,index_col=0)\n",
    "pred = pd.read_csv('./DataSet/book/golden/GCNResult.txt',sep='\\t',low_memory=False,index_col=0)\n",
    "\n",
    "print(JudgeAccu(label,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meadors, Todd; Schmidt, Cheryl Ann vs meadors, todd ;  schmidt, cheryl a.;   0.6666666666666666\n",
      "C Bala Kumar, Paul Kline, Tim Thompson vs kumar, c bala;  kline, paul j.;  thompson, tim j.;   0.875\n",
      "Hoos, Holger vs hoos, holger h.;   stutzle, thomas;   0.4\n",
      "Loshin, Peter vs loshin, pete ;   0.3333333333333333\n",
      "Eberhart, Russell vs kennedy, james ;  eberhart, russell c.;   0.4\n",
      "Edited by Jones, Karen Sparck Edited by Willett, Peter vs jones, karen sparck;  willett, peter ;   0.7142857142857143\n",
      "Widom, Jennifer vs widom, jennifer ;  ceri, stefano; 0.5\n",
      "Fortier, Paul J.; Michel, Howard vs fortier, paul j.;  michel, howard e.;   0.8333333333333334\n",
      "Guy Steele vs steele, guy l.;   0.6666666666666666\n",
      "Jenssen, C. B. vs jenssen, c. b.;  kvamdal, t. ;  andersson, h. i.;  Ecer, A.;  Periaux, J.;  Satofuka, N.;  Fox, P.; 0.1875\n",
      "Dowd, Mark; McDonald, John vs dowd, mark ;  mcdonald, john ;  schuh, justin ;   0.6666666666666666\n",
      "George F. Coulouris , Jean Dollimore , Tim Kindberg vs coulouris, george ;  dollimore, jean ;  kindberg, tim ;   0.8571428571428571\n",
      "W. Richard Stevens vs stevens, w. richard;  wright, gary r. ;   0.5\n",
      "MUSSER, DAVID R. - GILLMER J. DERGE - ATUL SAINI vs musser, david r.;  derge, gillmer j.;  saini, atul ;   0.8888888888888888\n",
      "Hornick, Mark F. vs hornick, mark f.;  marcade, erik ;  venkayala, sunil ;   0.42857142857142855\n",
      "Aiken, Peter vs aiken, peter ;  allen, m. david;   0.4\n",
      "(0.7333333333333333, 0.8886342592592592)\n"
     ]
    }
   ],
   "source": [
    "df_mv = MV(data_withConfidence[test_mask.numpy()],withWeight=False)\n",
    "df_mv.to_csv( './DataSet/book/golden/MVResult.txt' , sep='\\t' , index=False )\n",
    "\n",
    "label = pd.read_csv('./DataSet/book/book_golden.txt',sep='\\t',low_memory=False,names=['isbn','author'],header=None,index_col=0)\n",
    "pred = pd.read_csv('./DataSet/book/golden/MVResult.txt',sep='\\t',low_memory=False,index_col=0)\n",
    "\n",
    "print(JudgeAccu(label,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
