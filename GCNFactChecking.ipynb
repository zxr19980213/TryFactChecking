{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import difflib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl import DGLGraph\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#来自DGLGraph tutorial，in_feats、out_feats需要根据feature长度和分类数改动改动\n",
    "class NodeApplyModule(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        #self.conv = nn.Conv1d(in_feats,out_feats,1)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, node):\n",
    "        #h = self.conv(node.data['h'])\n",
    "        h = self.linear(node.data['h'])\n",
    "        if self.activation is not None:\n",
    "            h = self.activation(h,inplace=True)\n",
    "        return {'h' : h}\n",
    "\n",
    "gcn_msg = fn.copy_src(src='h', out='m')\n",
    "gcn_reduce = fn.mean(msg='m', out='h')\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(gcn_msg, gcn_reduce)\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #self.gcn1 = GCN(605, 384, F.relu)\n",
    "        #self.gcn1 = GCN(2304, 384, F.relu)\n",
    "        #self.gcn1 = GCN(768, 384, F.relu)\n",
    "        #self.gcn2 = GCN(384, 192, F.relu)\n",
    "        #self.fc = nn.Linear(192, 2)\n",
    "        self.fc1_1 = nn.Linear(768,96)\n",
    "        self.fc1_2 = nn.Linear(768,96)\n",
    "        self.fc1_3 = nn.Linear(768,96)\n",
    "        #self.gcn2 = GCN(288,96,F.relu)\n",
    "        #self.gcn3 = GCN(96,96,F.relu)\n",
    "        self.fc4 = nn.Linear(288,2)\n",
    "        \n",
    "    def forward(self, g, features):\n",
    "        x1 = self.fc1_1(features[:,0:768])\n",
    "        x2 = self.fc1_2(features[:,768:768*2])\n",
    "        x3 = self.fc1_3(features[:,768*2:768*3])\n",
    "        x = torch.cat((x1,x2,x3),1)\n",
    "        #x = self.gcn2(g,x)\n",
    "        #x = self.gcn3(g,x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, pred_prob):\n",
    "        #self.eval()\n",
    "        pred = F.softmax(pred_prob)\n",
    "        ans = []\n",
    "        for t in pred:\n",
    "            if t[0]>t[1]:\n",
    "                ans.append(0)\n",
    "            else:\n",
    "                ans.append(1)\n",
    "        return torch.tensor(ans)\n",
    "    '''\n",
    "    def predict(self, g, features):\n",
    "        #self.eval()\n",
    "        pred = F.softmax(self.forward(g, features))\n",
    "        ans = []\n",
    "        for t in pred:\n",
    "            if t[0]>t[1]:\n",
    "                ans.append(0)\n",
    "            else:\n",
    "                ans.append(1)\n",
    "        return torch.tensor(ans)\n",
    "    '''\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读取DealWithData中golden数据\n",
    "data_golden = pd.read_csv( './DataSet/book/golden/claims_golden2.txt' , sep='\\t' )\n",
    "data_golden['encode'] = torch.load('./DataSet/book/golden/claims_golden_encode_v2.pt')\n",
    "#data_golden['encode'] = torch.load('./DataSet/book/golden/claims_golden_encode.pt')\n",
    "GoldenLabel = pd.read_table(\"./DataSet/book/book_golden.txt\" , sep='\\t' , header=None , names=['isbn','author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#随意点划分训练集和测试集，直觉要根据'isbn'划分 -> 改为不直接划分数据集，采用mask方式划分\n",
    "'''\n",
    "data_train = pd.DataFrame(data_golden.drop(data_golden.index,inplace=False))\n",
    "data_test = pd.DataFrame(data_golden.drop(data_golden.index,inplace=False))\n",
    "for i in range(0,int(len(GoldenLabel)/2+1)):\n",
    "    data_trainSlice = data_golden[data_golden['isbn']==GoldenLabel.loc[i]['isbn']]\n",
    "    data_train = data_train.append(data_trainSlice)\n",
    "for i in range(int(len(GoldenLabel)/2+1),len(GoldenLabel)):\n",
    "    data_testSlice = data_golden[data_golden['isbn']==GoldenLabel.loc[i]['isbn']]\n",
    "    data_test = data_test.append(data_testSlice)\n",
    "data_train.reset_index(drop=True,inplace=True)\n",
    "data_test.reset_index(drop=True,inplace=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#使用遮盖而非分隔的方式划分数据集\n",
    "def divide_dataset(dfw,dfk,test_ratio):\n",
    "    a = np.random.choice(len(dfk), int(len(dfk)*test_ratio), replace=False)\n",
    "    test_set = set()\n",
    "    for i in range(0,a.shape[0]):\n",
    "        test_set.add(dfk.loc[a[i]]['isbn'])\n",
    "    train_mask = torch.Tensor(size=[len(dfw)]).bool()\n",
    "    test_mask = torch.Tensor(size=[len(dfw)]).bool()\n",
    "    for i in range(0,len(dfw)):\n",
    "        if dfw.loc[i]['isbn'] in test_set:\n",
    "            test_mask[i] = True\n",
    "            train_mask[i] = False\n",
    "        else:\n",
    "            test_mask[i] = False\n",
    "            train_mask[i] = True\n",
    "    return train_mask,test_mask\n",
    "train_mask,test_mask = divide_dataset(data_golden,GoldenLabel,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#构建图结构函数，根据训练集和测试集连边 -> 改为基于全体数据连边\n",
    "#此处可优化复杂度？n**2/2 -> kn\n",
    "#此处可根据'book_name'相似度进一步加边\n",
    "\n",
    "#使用Jaccard相似度之后边数 124446 -> 124468 ，影响不大？\n",
    "\n",
    "def sim_Jaccard (str1,str2) :\n",
    "    set1 = set( str1.lower().replace(';',' ').replace(',',' ').replace('.',' ').replace(':',' ').replace('&',' ').\n",
    "               replace('/',' ').replace('\\'',' ').replace('(author)',' ').replace('(joint author)',' ').split() )\n",
    "    set2 = set( str2.lower().replace(';',' ').replace(',',' ').replace('.',' ').replace(':',' ').replace('&',' ').\n",
    "               replace('/',' ').replace('\\'',' ').replace('(author)',' ').replace('(joint author)',' ').split() )\n",
    "    return len(set1&set2)/len(set1|set2)\n",
    "\n",
    "def generate_DGLGraph(df):\n",
    "    #edge_norm = []\n",
    "    g = DGLGraph()\n",
    "    g.add_nodes(df.shape[0])\n",
    "    for i in range(0,len(df)):\n",
    "        for j in range(i+1,len(df)):\n",
    "            if (df.loc[i][\"source\"]==df.loc[j][\"source\"]):\n",
    "                g.add_edge(i,j)\n",
    "                #edge_norm.append(1.0)\n",
    "                g.add_edge(j,i)\n",
    "                #edge_norm.append(1.0)\n",
    "            elif (df.loc[i][\"isbn\"]==df.loc[j][\"isbn\"]):\n",
    "                str1 = df.loc[i]['author']\n",
    "                str2 = df.loc[j]['author']\n",
    "                #print(str1,str2,difflib.SequenceMatcher(None,str1,str2).quick_ratio())\n",
    "                #if ( difflib.SequenceMatcher(None,str1,str2).quick_ratio()>0.8 ):\n",
    "                if ( sim_Jaccard(str1,str2)>0.8 ):\n",
    "                    g.add_edge(i,j)\n",
    "                    #edge_norm.append(1.0)\n",
    "                    g.add_edge(j,i)\n",
    "                    #edge_norm.append(1.0)\n",
    "    #edge_norm = torch.Tensor(edge_norm).unsqueeze(1)\n",
    "    #g.edata.update({ 'norm': edge_norm })\n",
    "    return g\n",
    "\n",
    "graph_whole = generate_DGLGraph(data_golden)\n",
    "#graph_train = generate_DGLGraph(data_train)\n",
    "#graph_test = generate_DGLGraph(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#存储图结构\n",
    "file = open('./DataSet/book/golden/graph_whole2.pickle', 'wb')\n",
    "pickle.dump(graph_whole, file)\n",
    "file.close()\n",
    "#file = open('./DataSet/book/golden/graph_train.pickle', 'wb')\n",
    "#pickle.dump(graph_train, file)\n",
    "#file.close()\n",
    "#file = open('./DataSet/book/golden/graph_test.pickle', 'wb')\n",
    "#pickle.dump(graph_test, file)\n",
    "#file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加载图结构\n",
    "with open('./DataSet/book/golden/graph_whole2.pickle', 'rb') as file:\n",
    "    graph_whole =pickle.load(file)\n",
    "#with open('./DataSet/book/golden/graph_train.pickle', 'rb') as file:\n",
    "#    graph_train =pickle.load(file)\n",
    "#with open('./DataSet/book/golden/graph_test.pickle', 'rb') as file:\n",
    "#    graph_test =pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#存储和加载图结构的失败尝试\n",
    "'''\n",
    "graph_train_netx = graph_train.to_networkx()\n",
    "graph_test_netx = graph_test.to_networkx()\n",
    "nx.write_gexf(graph_train_netx,'./DataSet/book/golden/train_graph.gexf')\n",
    "nx.write_gexf(graph_test_netx,'./DataSet/book/golden/test_graph.gexf')\n",
    "\n",
    "graph_train_netx = nx.read_gexf('./DataSet/book/golden/train_graph.gexf')\n",
    "graph_test_netx = nx.read_gexf('./DataSet/book/golden/test_graph.gexf')\n",
    "\n",
    "graph_train2 = DGLGraph(graph_train_netx)\n",
    "#graph_train2.from_networkx(graph_train_netx)\n",
    "graph_test2 = DGLGraph(graph_test_netx)\n",
    "#graph_test2.from_networkx(graph_test_netx)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_featureNlabel(df):\n",
    "    a = torch.zeros(1,len(df.loc[0]['encode']))\n",
    "    c = torch.zeros(1)\n",
    "    for _,row in df.iterrows():\n",
    "        b = row['encode'].reshape([1,-1])\n",
    "        a = torch.cat((a,b),0)\n",
    "        if(row['label']):\n",
    "            d = torch.ones(1)\n",
    "        else:\n",
    "            d = torch.zeros(1)\n",
    "        c = torch.cat((c,d),-1)\n",
    "    return a[1:,:],c[1:].long()\n",
    "#_,whole_label = extract_featureNlabel(data_golden)\n",
    "#whole_feature = torch.load('./DataSet/book/golden/claims_golden_encode_tfidf.pt')\n",
    "whole_feature,whole_label = extract_featureNlabel(data_golden)\n",
    "#train_feature,train_label = extract_featureNlabel(data_train)\n",
    "#test_feature,test_label = extract_featureNlabel(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果比对\n",
    "\n",
    "a:1\\*768(bert)\n",
    "\n",
    "b:1\\*3\\*768(bert_v2)\n",
    "\n",
    "c:1\\*605(tfidf)\n",
    "\n",
    "train:test=1:1\n",
    "\n",
    "| method | len | lr | epoch | loss | accu |\n",
    "|:----: | :----: | :----: | :----: | :----: | :----: |\n",
    "| a | 768 | 1e-5 | 200 | 1.xx | 0.6 | \n",
    "| b | 2304 | 1e-5 | 200 | 5.xx | 0.7 |\n",
    "| c | 605 | 1e-5 | 200 | 0.5x | 0.7 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\acaconda\\envs\\pytorch_gpu\\lib\\site-packages\\ipykernel_launcher.py:57: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 0.7364 | Time(s) 0.1652 | Train_Accu 0.523385 | Test_Accu 0.444573\n",
      "Epoch 00001 | Loss 0.6816 | Time(s) 0.1610 | Train_Accu 0.567929 | Test_Accu 0.559469\n",
      "Epoch 00002 | Loss 0.7011 | Time(s) 0.1586 | Train_Accu 0.545657 | Test_Accu 0.588915\n",
      "Epoch 00003 | Loss 0.6578 | Time(s) 0.1535 | Train_Accu 0.594655 | Test_Accu 0.583718\n",
      "Epoch 00004 | Loss 0.6308 | Time(s) 0.1542 | Train_Accu 0.641425 | Test_Accu 0.516744\n",
      "Epoch 00005 | Loss 0.6363 | Time(s) 0.1549 | Train_Accu 0.625835 | Test_Accu 0.484988\n",
      "Epoch 00006 | Loss 0.6348 | Time(s) 0.1531 | Train_Accu 0.628062 | Test_Accu 0.472286\n",
      "Epoch 00007 | Loss 0.6139 | Time(s) 0.1522 | Train_Accu 0.663697 | Test_Accu 0.498845\n",
      "Epoch 00008 | Loss 0.5931 | Time(s) 0.1530 | Train_Accu 0.681514 | Test_Accu 0.544457\n",
      "Epoch 00009 | Loss 0.5867 | Time(s) 0.1529 | Train_Accu 0.685969 | Test_Accu 0.595266\n",
      "Epoch 00010 | Loss 0.5873 | Time(s) 0.1539 | Train_Accu 0.679287 | Test_Accu 0.623557\n",
      "Epoch 00011 | Loss 0.5809 | Time(s) 0.1540 | Train_Accu 0.690423 | Test_Accu 0.625289\n",
      "Epoch 00012 | Loss 0.5667 | Time(s) 0.1591 | Train_Accu 0.692650 | Test_Accu 0.617206\n",
      "Epoch 00013 | Loss 0.5539 | Time(s) 0.1596 | Train_Accu 0.710468 | Test_Accu 0.593533\n",
      "Epoch 00014 | Loss 0.5485 | Time(s) 0.1589 | Train_Accu 0.719376 | Test_Accu 0.568707\n",
      "Epoch 00015 | Loss 0.5464 | Time(s) 0.1600 | Train_Accu 0.734967 | Test_Accu 0.555427\n",
      "Epoch 00016 | Loss 0.5409 | Time(s) 0.1599 | Train_Accu 0.741648 | Test_Accu 0.547921\n",
      "Epoch 00017 | Loss 0.5310 | Time(s) 0.1602 | Train_Accu 0.746102 | Test_Accu 0.570439\n",
      "Epoch 00018 | Loss 0.5214 | Time(s) 0.1603 | Train_Accu 0.739421 | Test_Accu 0.590069\n",
      "Epoch 00019 | Loss 0.5157 | Time(s) 0.1619 | Train_Accu 0.741648 | Test_Accu 0.615473\n",
      "Epoch 00020 | Loss 0.5125 | Time(s) 0.1616 | Train_Accu 0.748330 | Test_Accu 0.627021\n",
      "Epoch 00021 | Loss 0.5078 | Time(s) 0.1610 | Train_Accu 0.748330 | Test_Accu 0.636259\n",
      "Epoch 00022 | Loss 0.5004 | Time(s) 0.1603 | Train_Accu 0.768374 | Test_Accu 0.626443\n",
      "Epoch 00023 | Loss 0.4927 | Time(s) 0.1598 | Train_Accu 0.775056 | Test_Accu 0.613741\n",
      "Epoch 00024 | Loss 0.4873 | Time(s) 0.1602 | Train_Accu 0.777283 | Test_Accu 0.602771\n",
      "Epoch 00025 | Loss 0.4836 | Time(s) 0.1598 | Train_Accu 0.790646 | Test_Accu 0.594688\n",
      "Epoch 00026 | Loss 0.4794 | Time(s) 0.1592 | Train_Accu 0.792873 | Test_Accu 0.592956\n",
      "Epoch 00027 | Loss 0.4736 | Time(s) 0.1590 | Train_Accu 0.797327 | Test_Accu 0.598152\n",
      "Epoch 00028 | Loss 0.4674 | Time(s) 0.1587 | Train_Accu 0.799555 | Test_Accu 0.606813\n",
      "Epoch 00029 | Loss 0.4625 | Time(s) 0.1582 | Train_Accu 0.801782 | Test_Accu 0.627021\n",
      "Epoch 00030 | Loss 0.4589 | Time(s) 0.1600 | Train_Accu 0.806236 | Test_Accu 0.635104\n",
      "Epoch 00031 | Loss 0.4550 | Time(s) 0.1607 | Train_Accu 0.801782 | Test_Accu 0.636836\n",
      "Epoch 00032 | Loss 0.4502 | Time(s) 0.1604 | Train_Accu 0.806236 | Test_Accu 0.637413\n",
      "Epoch 00033 | Loss 0.4451 | Time(s) 0.1600 | Train_Accu 0.812918 | Test_Accu 0.631640\n",
      "Epoch 00034 | Loss 0.4408 | Time(s) 0.1608 | Train_Accu 0.821826 | Test_Accu 0.620670\n",
      "Epoch 00035 | Loss 0.4373 | Time(s) 0.1609 | Train_Accu 0.817372 | Test_Accu 0.621247\n",
      "Epoch 00036 | Loss 0.4337 | Time(s) 0.1606 | Train_Accu 0.828508 | Test_Accu 0.623557\n",
      "Epoch 00037 | Loss 0.4295 | Time(s) 0.1613 | Train_Accu 0.830735 | Test_Accu 0.622979\n",
      "Epoch 00038 | Loss 0.4252 | Time(s) 0.1610 | Train_Accu 0.821826 | Test_Accu 0.625289\n",
      "Epoch 00039 | Loss 0.4214 | Time(s) 0.1608 | Train_Accu 0.826281 | Test_Accu 0.633372\n",
      "Epoch 00040 | Loss 0.4181 | Time(s) 0.1620 | Train_Accu 0.826281 | Test_Accu 0.636836\n",
      "Epoch 00041 | Loss 0.4147 | Time(s) 0.1620 | Train_Accu 0.832962 | Test_Accu 0.643187\n",
      "Epoch 00042 | Loss 0.4109 | Time(s) 0.1618 | Train_Accu 0.830735 | Test_Accu 0.641455\n",
      "Epoch 00043 | Loss 0.4072 | Time(s) 0.1616 | Train_Accu 0.839644 | Test_Accu 0.635681\n",
      "Epoch 00044 | Loss 0.4038 | Time(s) 0.1618 | Train_Accu 0.839644 | Test_Accu 0.633372\n",
      "Epoch 00045 | Loss 0.4007 | Time(s) 0.1619 | Train_Accu 0.841871 | Test_Accu 0.635104\n",
      "Epoch 00046 | Loss 0.3974 | Time(s) 0.1636 | Train_Accu 0.846325 | Test_Accu 0.633949\n",
      "Epoch 00047 | Loss 0.3940 | Time(s) 0.1637 | Train_Accu 0.846325 | Test_Accu 0.636836\n",
      "Epoch 00048 | Loss 0.3907 | Time(s) 0.1637 | Train_Accu 0.850780 | Test_Accu 0.642610\n",
      "Epoch 00049 | Loss 0.3877 | Time(s) 0.1634 | Train_Accu 0.857461 | Test_Accu 0.647229\n",
      "Epoch 00050 | Loss 0.3847 | Time(s) 0.1638 | Train_Accu 0.859688 | Test_Accu 0.650693\n",
      "Epoch 00051 | Loss 0.3817 | Time(s) 0.1634 | Train_Accu 0.864143 | Test_Accu 0.651848\n",
      "Epoch 00052 | Loss 0.3786 | Time(s) 0.1634 | Train_Accu 0.866370 | Test_Accu 0.647806\n",
      "Epoch 00053 | Loss 0.3756 | Time(s) 0.1640 | Train_Accu 0.864143 | Test_Accu 0.645497\n",
      "Epoch 00054 | Loss 0.3728 | Time(s) 0.1640 | Train_Accu 0.868597 | Test_Accu 0.642610\n",
      "Epoch 00055 | Loss 0.3700 | Time(s) 0.1642 | Train_Accu 0.870824 | Test_Accu 0.642610\n",
      "Epoch 00056 | Loss 0.3671 | Time(s) 0.1644 | Train_Accu 0.870824 | Test_Accu 0.642610\n",
      "Epoch 00057 | Loss 0.3643 | Time(s) 0.1642 | Train_Accu 0.881960 | Test_Accu 0.646651\n",
      "Epoch 00058 | Loss 0.3615 | Time(s) 0.1646 | Train_Accu 0.881960 | Test_Accu 0.648961\n",
      "Epoch 00059 | Loss 0.3589 | Time(s) 0.1648 | Train_Accu 0.881960 | Test_Accu 0.651848\n",
      "Epoch 00060 | Loss 0.3562 | Time(s) 0.1645 | Train_Accu 0.888641 | Test_Accu 0.650115\n",
      "Epoch 00061 | Loss 0.3535 | Time(s) 0.1651 | Train_Accu 0.893096 | Test_Accu 0.649538\n",
      "Epoch 00062 | Loss 0.3509 | Time(s) 0.1650 | Train_Accu 0.890869 | Test_Accu 0.647229\n",
      "Epoch 00063 | Loss 0.3483 | Time(s) 0.1648 | Train_Accu 0.890869 | Test_Accu 0.646651\n",
      "Epoch 00064 | Loss 0.3458 | Time(s) 0.1651 | Train_Accu 0.893096 | Test_Accu 0.647806\n",
      "Epoch 00065 | Loss 0.3433 | Time(s) 0.1649 | Train_Accu 0.893096 | Test_Accu 0.648961\n",
      "Epoch 00066 | Loss 0.3407 | Time(s) 0.1652 | Train_Accu 0.893096 | Test_Accu 0.648383\n",
      "Epoch 00067 | Loss 0.3383 | Time(s) 0.1654 | Train_Accu 0.899777 | Test_Accu 0.650693\n",
      "Epoch 00068 | Loss 0.3359 | Time(s) 0.1650 | Train_Accu 0.899777 | Test_Accu 0.650693\n",
      "Epoch 00069 | Loss 0.3335 | Time(s) 0.1652 | Train_Accu 0.902004 | Test_Accu 0.651270\n",
      "Epoch 00070 | Loss 0.3311 | Time(s) 0.1657 | Train_Accu 0.902004 | Test_Accu 0.651848\n",
      "Epoch 00071 | Loss 0.3287 | Time(s) 0.1655 | Train_Accu 0.899777 | Test_Accu 0.650115\n",
      "Epoch 00072 | Loss 0.3264 | Time(s) 0.1659 | Train_Accu 0.897550 | Test_Accu 0.649538\n",
      "Epoch 00073 | Loss 0.3241 | Time(s) 0.1658 | Train_Accu 0.897550 | Test_Accu 0.650115\n",
      "Epoch 00074 | Loss 0.3217 | Time(s) 0.1657 | Train_Accu 0.899777 | Test_Accu 0.651270\n",
      "Epoch 00075 | Loss 0.3195 | Time(s) 0.1661 | Train_Accu 0.902004 | Test_Accu 0.653002\n",
      "Epoch 00076 | Loss 0.3172 | Time(s) 0.1662 | Train_Accu 0.906459 | Test_Accu 0.651270\n",
      "Epoch 00077 | Loss 0.3150 | Time(s) 0.1659 | Train_Accu 0.910913 | Test_Accu 0.652425\n",
      "Epoch 00078 | Loss 0.3128 | Time(s) 0.1663 | Train_Accu 0.910913 | Test_Accu 0.652425\n",
      "Epoch 00079 | Loss 0.3106 | Time(s) 0.1662 | Train_Accu 0.908686 | Test_Accu 0.651848\n",
      "Epoch 00080 | Loss 0.3084 | Time(s) 0.1660 | Train_Accu 0.908686 | Test_Accu 0.649538\n",
      "Epoch 00081 | Loss 0.3063 | Time(s) 0.1657 | Train_Accu 0.908686 | Test_Accu 0.648383\n",
      "Epoch 00082 | Loss 0.3042 | Time(s) 0.1660 | Train_Accu 0.908686 | Test_Accu 0.649538\n",
      "Epoch 00083 | Loss 0.3020 | Time(s) 0.1659 | Train_Accu 0.913140 | Test_Accu 0.650693\n",
      "Epoch 00084 | Loss 0.3000 | Time(s) 0.1659 | Train_Accu 0.915367 | Test_Accu 0.654157\n",
      "Epoch 00085 | Loss 0.2979 | Time(s) 0.1660 | Train_Accu 0.917595 | Test_Accu 0.654734\n",
      "Epoch 00086 | Loss 0.2958 | Time(s) 0.1658 | Train_Accu 0.917595 | Test_Accu 0.654734\n",
      "Epoch 00087 | Loss 0.2938 | Time(s) 0.1656 | Train_Accu 0.917595 | Test_Accu 0.653002\n",
      "Epoch 00088 | Loss 0.2918 | Time(s) 0.1660 | Train_Accu 0.915367 | Test_Accu 0.651270\n",
      "Epoch 00089 | Loss 0.2898 | Time(s) 0.1667 | Train_Accu 0.915367 | Test_Accu 0.651848\n",
      "Epoch 00090 | Loss 0.2878 | Time(s) 0.1671 | Train_Accu 0.915367 | Test_Accu 0.651848\n",
      "Epoch 00091 | Loss 0.2858 | Time(s) 0.1670 | Train_Accu 0.917595 | Test_Accu 0.651848\n",
      "Epoch 00092 | Loss 0.2838 | Time(s) 0.1669 | Train_Accu 0.917595 | Test_Accu 0.652425\n",
      "Epoch 00093 | Loss 0.2819 | Time(s) 0.1667 | Train_Accu 0.917595 | Test_Accu 0.653002\n",
      "Epoch 00094 | Loss 0.2800 | Time(s) 0.1667 | Train_Accu 0.919822 | Test_Accu 0.652425\n",
      "Epoch 00095 | Loss 0.2781 | Time(s) 0.1666 | Train_Accu 0.919822 | Test_Accu 0.654157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00096 | Loss 0.2762 | Time(s) 0.1666 | Train_Accu 0.919822 | Test_Accu 0.652425\n",
      "Epoch 00097 | Loss 0.2743 | Time(s) 0.1664 | Train_Accu 0.919822 | Test_Accu 0.653580\n",
      "Epoch 00098 | Loss 0.2724 | Time(s) 0.1664 | Train_Accu 0.919822 | Test_Accu 0.653580\n",
      "Epoch 00099 | Loss 0.2706 | Time(s) 0.1664 | Train_Accu 0.919822 | Test_Accu 0.655312\n"
     ]
    }
   ],
   "source": [
    "dur = []\n",
    "for epoch in range(100):\n",
    "    \n",
    "    t0 = time.time()\n",
    "\n",
    "    pred_prob = net.forward(graph_whole, whole_feature)\n",
    "    loss = criterion(pred_prob[train_mask],whole_label[train_mask])\n",
    "    \n",
    "    #pred_prob = net.forward(graph_train, train_feature)\n",
    "    #loss = criterion(pred_prob,train_label)\n",
    "    #pred_label = net.predict(graph_whole, whole_feature)\n",
    "    pred_label = net.predict(pred_prob)\n",
    "    train_accu = accuracy_score(pred_label[train_mask],whole_label[train_mask])\n",
    "    test_accu = accuracy_score(pred_label[test_mask],whole_label[test_mask])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    dur.append(time.time() - t0)\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Time(s) {:.4f} | Train_Accu {:4f} | Test_Accu {:4f}\".format(\n",
    "        epoch, loss.item(), np.mean(dur), train_accu,test_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#玩具数据集用以检验图结构生成的准确率。\n",
    "#测试发现difflib的相似度量是字符级的\n",
    "'''\n",
    "dataframe = pd.DataFrame([\n",
    "        [\"a\", \"111222\", \"computer Science\", \"bruce\"],\n",
    "        [\"b\", \"111222\", \"computer Science\", \"Bruce Lee\"],\n",
    "        [\"c\", \"111222\", \"computer Science\", \"mike ,john\"],\n",
    "        [\"a\", \"111223\", \"Hassdsdsaad\", \"kkl\"],\n",
    "        [\"d\", \"111223\", \"Hassdsdaaad\", \"kkkl\"],\n",
    "        [\"c\", \"111224\", \"asdfgh\", \"zxcr\"]\n",
    "    ],\n",
    "    columns=[\"source\", \"isbn\", \"name\", \"author\"]\n",
    ")\n",
    "g = generate_DGLGraph(dataframe)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\acaconda\\envs\\pytorch_gpu\\lib\\site-packages\\ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8625\n"
     ]
    }
   ],
   "source": [
    "def add_confidence(df,prob,col_name='fact_confidence'):\n",
    "    df[col_name] = None\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i,col_name] = float(prob[i][1])\n",
    "    return df\n",
    "\n",
    "def sim_Jaccard (str1,str2) :\n",
    "    set1 = set( str1.lower().replace(';',' ').replace(',',' ').replace('.',' ').replace(':',' ').replace('&',' ').\n",
    "               replace('/',' ').replace('\\'',' ').replace('(author)',' ').replace('(joint author)',' ').split() )\n",
    "    set2 = set( str2.lower().replace(';',' ').replace(',',' ').replace('.',' ').replace(':',' ').replace('&',' ').\n",
    "               replace('/',' ').replace('\\'',' ').replace('(author)',' ').replace('(joint author)',' ').split() )\n",
    "    return len(set1&set2)/len(set1|set2)\n",
    "\n",
    "def MV(df,indexK='isbn',answer='author',withWeight=False,weight='confidence'):\n",
    "    df_mv = pd.DataFrame(columns=[indexK,answer])\n",
    "    for indexV in df[indexK].unique():\n",
    "        data_slice = df[df[indexK]==indexV]\n",
    "        vote_dict = {}\n",
    "        for index,row in data_slice.iterrows():\n",
    "            flag = False\n",
    "            for key in vote_dict.keys():\n",
    "                if ( sim_Jaccard(key,row[answer])>=0.8 ):\n",
    "                    flag = True\n",
    "                    if(not withWeight):\n",
    "                        vote_dict[key] += 1\n",
    "                    else:\n",
    "                        vote_dict[key] += float(row[weight])\n",
    "                    break\n",
    "            if (not flag):\n",
    "                if(not withWeight):\n",
    "                    vote_dict[row[answer]] = 1\n",
    "                else:\n",
    "                    vote_dict[row[answer]] = float(row[weight])\n",
    "        vote_list = sorted(vote_dict.items(), key=lambda d:d[1],reverse=True)\n",
    "        #print({indexK:indexV,answer:vote_list[0][0]})\n",
    "        df_mv = df_mv.append({indexK:indexV,answer:vote_list[0][0]},ignore_index=True)\n",
    "    return df_mv\n",
    "\n",
    "def JudgeAccu(label,pred,pred_col='author'):\n",
    "    score = 0\n",
    "    for index,row in pred.iterrows():\n",
    "        if not(index in label.index):\n",
    "            print(index,'no answer')\n",
    "            score += 0 \n",
    "        elif sim_Jaccard(row[pred_col],label.loc[index][pred_col])>=0.8:\n",
    "            score +=1\n",
    "        else:\n",
    "            #print(row[pred_col],\"vs\",label.loc[index][pred_col])\n",
    "            score += 0\n",
    "    return score/len(pred)\n",
    "\n",
    "data_withConfidence = add_confidence(data_golden,F.softmax(pred_prob))\n",
    "\n",
    "df_mv = MV(data_withConfidence[test_mask.numpy()],withWeight=True,weight='fact_confidence')\n",
    "df_mv.to_csv( './DataSet/book/golden/GCNResult.txt' , sep='\\t' , index=False )\n",
    "\n",
    "label = pd.read_csv('./DataSet/book/book_golden.txt',sep='\\t',low_memory=False,names=['isbn','author'],header=None,index_col=0)\n",
    "pred = pd.read_csv('./DataSet/book/golden/GCNResult.txt',sep='\\t',low_memory=False,index_col=0)\n",
    "\n",
    "print(JudgeAccu(label,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7875\n"
     ]
    }
   ],
   "source": [
    "df_mv = MV(data_withConfidence[test_mask.numpy()],withWeight=False)\n",
    "df_mv.to_csv( './DataSet/book/golden/MVResult.txt' , sep='\\t' , index=False )\n",
    "\n",
    "label = pd.read_csv('./DataSet/book/book_golden.txt',sep='\\t',low_memory=False,names=['isbn','author'],header=None,index_col=0)\n",
    "pred = pd.read_csv('./DataSet/book/golden/MVResult.txt',sep='\\t',low_memory=False,index_col=0)\n",
    "\n",
    "print(JudgeAccu(label,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_gpu]",
   "language": "python",
   "name": "conda-env-pytorch_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
