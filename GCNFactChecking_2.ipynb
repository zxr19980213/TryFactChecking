{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import difflib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl import DGLGraph\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    net1: 无自环\n",
    "        self.fc1_1 = nn.Linear(768,192)\n",
    "        self.fc1_2 = nn.Linear(768,192)\n",
    "        self.fc1_3 = nn.Linear(768,192)\n",
    "        self.fc2 = nn.Linear(576,2)\n",
    "    50 epoch, train 0.75, test 0.63\n",
    "    \n",
    "    net2: 无自环\n",
    "        self.fc1_1 = nn.Linear(768,192)\n",
    "        self.fc1_2 = nn.Linear(768,192)\n",
    "        self.fc1_3 = nn.Linear(768,192)\n",
    "        self.gcn2 = GCN(576,384,F.relu)\n",
    "        self.fc3 = nn.Linear(384,2)\n",
    "    100 epoch, train 0.85, test 0.65\n",
    "    \n",
    "    net3: 无自环\n",
    "        self.fc1_1 = nn.Linear(768,192)\n",
    "        self.fc1_2 = nn.Linear(768,192)\n",
    "        self.fc1_3 = nn.Linear(768,192)\n",
    "        self.gcn2 = GCN(576,384,F.relu)\n",
    "        self.gcn3 = GCN(384,64,F.relu)\n",
    "        self.fc4 = nn.Linear(64,2)\n",
    "    100 epoch, train 0.75, test 0.60\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#来自DGLGraph tutorial，in_feats、out_feats需要根据feature长度和分类数改动改动\n",
    "class NodeApplyModule(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        if self.activation is not None:\n",
    "            h = self.activation(h,inplace=True)\n",
    "        return {'h' : h}\n",
    "\n",
    "gcn_msg = fn.copy_src(src='h', out='m')\n",
    "gcn_reduce = fn.sum(msg='m', out='h')\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(gcn_msg, gcn_reduce)\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1_1 = nn.Linear(768,96)\n",
    "        self.fc1_2 = nn.Linear(768,96)\n",
    "        self.fc1_3 = nn.Linear(768,96)\n",
    "        self.gcn2 = GCN(288,64,F.relu)\n",
    "        self.gcn3 = GCN(64,64,F.relu)\n",
    "        self.fc4 = nn.Linear(64,1)\n",
    "        \n",
    "    def forward(self,g,features):\n",
    "        x_1 = self.fc1_1(features[:,0:768])\n",
    "        x_2 = self.fc1_2(features[:,768:768*2])\n",
    "        x_3 = self.fc1_3(features[:,768*2:768*3])\n",
    "        x = torch.cat( ( x_1,x_2,x_3 ) , 1 )\n",
    "        x = self.gcn2(g,x)\n",
    "        x = self.gcn3(g,x)\n",
    "        x = self.fc4(x)\n",
    "        return x.squeeze()\n",
    "    \n",
    "    def predict(self, pred_prob):\n",
    "        #pred = F.softmax(pred_prob,axis=1)\n",
    "        pred = F.sigmoid(pred_prob)\n",
    "        ans = []\n",
    "        for t in pred:\n",
    "            ans.append(float(t>0.5))\n",
    "            #if t[0]>t[1]:\n",
    "            #    ans.append(0)\n",
    "            #else:\n",
    "            #    ans.append(1)\n",
    "        return torch.FloatTensor(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读取DealWithData中golden数据和label数据\n",
    "#该版本削减了同source不同big_cate下的边，得到正反馈的结果\n",
    "GoldenLabel = pd.read_table('./DataSet/vldbBook/book_truth.txt',low_memory=False)\n",
    "GoldenLabel.rename(columns={'isbn_10':'isbn','authors_truth':'author'},inplace=True)\n",
    "data_golden = pd.read_csv( './DataSet/vldbBook/data_golden/data_golden.txt'  , sep='\\t' )\n",
    "data_golden['encode'] = torch.load('./DataSet/vldbBook/data_golden/bertEncodeFull.pt')\n",
    "with open('./DataSet/vldbBook/data_golden/graph.pickle', 'rb') as file:\n",
    "    graph_whole =pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#使用遮盖而非分隔的方式划分数据集\n",
    "def divide_dataset(dfw,dfk,test_ratio):\n",
    "    a = np.random.choice(len(dfk), int(len(dfk)*test_ratio), replace=False)\n",
    "    test_set = set()\n",
    "    for i in range(0,a.shape[0]):\n",
    "        test_set.add(dfk.loc[a[i]]['isbn'])\n",
    "    train_mask = torch.Tensor(size=[len(dfw)]).bool()\n",
    "    test_mask = torch.Tensor(size=[len(dfw)]).bool()\n",
    "    for i in range(0,len(dfw)):\n",
    "        if dfw.loc[i]['isbn'] in test_set:\n",
    "            test_mask[i] = True\n",
    "            train_mask[i] = False\n",
    "        else:\n",
    "            test_mask[i] = False\n",
    "            train_mask[i] = True\n",
    "    return train_mask,test_mask\n",
    "train_mask,test_mask = divide_dataset(data_golden,GoldenLabel,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#从dataframe中提取feature以及label\n",
    "def extract_featureNlabel(df):\n",
    "    a = torch.zeros(1,len(df.loc[0]['encode']))\n",
    "    c = torch.zeros(1)\n",
    "    for _,row in df.iterrows():\n",
    "        b = row['encode'].reshape([1,-1])\n",
    "        a = torch.cat((a,b),0)\n",
    "        if(row['label']):\n",
    "            d = torch.ones(1)\n",
    "        else:\n",
    "            d = torch.zeros(1)\n",
    "        c = torch.cat((c,d),-1)\n",
    "    return a[1:,:],c[1:].long()\n",
    "\n",
    "whole_feature,whole_label = extract_featureNlabel(data_golden)\n",
    "#使用pickle将特征与标签存储在文件中以便复用\n",
    "file = open('./DataSet/vldbBook/data_golden/whole_feature.pickle', 'wb')\n",
    "pickle.dump(whole_feature, file)\n",
    "file.close()\n",
    "file = open('./DataSet/vldbBook/data_golden/whole_label.pickle', 'wb')\n",
    "pickle.dump(whole_label, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#从文件中读取提取出的特征和标签\n",
    "with open('./DataSet/vldbBook/data_golden/whole_feature.pickle', 'rb') as file:\n",
    "    whole_feature =pickle.load(file)\n",
    "with open('./DataSet/vldbBook/data_golden/whole_label.pickle', 'rb') as file:\n",
    "    whole_label =pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\acaconda\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "D:\\acaconda\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "D:\\acaconda\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GCN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "D:\\acaconda\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type NodeApplyModule. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(net,'./DataSet/vldbBook/data_golden/net.pkl')\n",
    "net = torch.load('./DataSet/vldbBook/data_golden/net.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss 47.5400 | Time(s) 2.1318 | Train_Accu 0.510790 | Test_Accu 0.527438\n",
      "Epoch 001 | Loss 1299.9746 | Time(s) 2.1430 | Train_Accu 0.509153 | Test_Accu 0.486559\n",
      "Epoch 002 | Loss 369.4326 | Time(s) 2.1457 | Train_Accu 0.509153 | Test_Accu 0.486559\n",
      "Epoch 003 | Loss 39.4337 | Time(s) 2.1722 | Train_Accu 0.521804 | Test_Accu 0.538769\n",
      "Epoch 004 | Loss 51.2407 | Time(s) 2.1667 | Train_Accu 0.510790 | Test_Accu 0.529660\n",
      "Epoch 005 | Loss 16.3463 | Time(s) 2.1648 | Train_Accu 0.450960 | Test_Accu 0.505221\n",
      "Epoch 006 | Loss 11.5075 | Time(s) 2.1611 | Train_Accu 0.534306 | Test_Accu 0.554543\n",
      "Epoch 007 | Loss 138.4835 | Time(s) 2.1623 | Train_Accu 0.509153 | Test_Accu 0.486559\n",
      "Epoch 008 | Loss 23.9365 | Time(s) 2.1602 | Train_Accu 0.524334 | Test_Accu 0.541213\n",
      "Epoch 009 | Loss 40.0762 | Time(s) 2.1545 | Train_Accu 0.521506 | Test_Accu 0.537880\n",
      "Epoch 010 | Loss 20.6567 | Time(s) 2.1558 | Train_Accu 0.527757 | Test_Accu 0.543657\n",
      "Epoch 011 | Loss 56.7509 | Time(s) 2.1548 | Train_Accu 0.509153 | Test_Accu 0.486559\n",
      "Epoch 012 | Loss 8.6324 | Time(s) 2.1533 | Train_Accu 0.581188 | Test_Accu 0.558320\n",
      "Epoch 013 | Loss 17.3613 | Time(s) 2.1525 | Train_Accu 0.548445 | Test_Accu 0.558320\n",
      "Epoch 014 | Loss 8.7340 | Time(s) 2.1513 | Train_Accu 0.641018 | Test_Accu 0.626972\n",
      "Epoch 015 | Loss 18.2408 | Time(s) 2.1492 | Train_Accu 0.627623 | Test_Accu 0.540547\n",
      "Epoch 016 | Loss 8.7792 | Time(s) 2.1490 | Train_Accu 0.651139 | Test_Accu 0.595423\n",
      "Epoch 017 | Loss 15.9694 | Time(s) 2.1471 | Train_Accu 0.599643 | Test_Accu 0.597423\n",
      "Epoch 018 | Loss 7.2774 | Time(s) 2.1504 | Train_Accu 0.644292 | Test_Accu 0.600978\n",
      "Epoch 019 | Loss 16.7773 | Time(s) 2.1515 | Train_Accu 0.626433 | Test_Accu 0.505221\n",
      "Epoch 020 | Loss 9.6992 | Time(s) 2.1507 | Train_Accu 0.633874 | Test_Accu 0.607198\n",
      "Epoch 021 | Loss 11.5529 | Time(s) 2.1534 | Train_Accu 0.623307 | Test_Accu 0.606532\n",
      "Epoch 022 | Loss 6.5525 | Time(s) 2.1584 | Train_Accu 0.678375 | Test_Accu 0.559209\n",
      "Epoch 023 | Loss 12.3035 | Time(s) 2.1596 | Train_Accu 0.661854 | Test_Accu 0.515441\n",
      "Epoch 024 | Loss 6.2068 | Time(s) 2.1629 | Train_Accu 0.636404 | Test_Accu 0.634970\n",
      "Epoch 025 | Loss 8.9439 | Time(s) 2.1640 | Train_Accu 0.630749 | Test_Accu 0.617418\n",
      "Epoch 026 | Loss 5.0280 | Time(s) 2.1633 | Train_Accu 0.688198 | Test_Accu 0.614530\n",
      "Epoch 027 | Loss 7.9768 | Time(s) 2.1625 | Train_Accu 0.698021 | Test_Accu 0.523439\n",
      "Epoch 028 | Loss 4.5068 | Time(s) 2.1658 | Train_Accu 0.654413 | Test_Accu 0.609420\n",
      "Epoch 029 | Loss 5.4347 | Time(s) 2.1685 | Train_Accu 0.643697 | Test_Accu 0.628971\n",
      "Epoch 030 | Loss 6.5453 | Time(s) 2.1699 | Train_Accu 0.704123 | Test_Accu 0.513664\n",
      "Epoch 031 | Loss 2.7045 | Time(s) 2.1839 | Train_Accu 0.705611 | Test_Accu 0.640524\n",
      "Epoch 032 | Loss 7.0289 | Time(s) 2.1864 | Train_Accu 0.620182 | Test_Accu 0.614974\n",
      "Epoch 033 | Loss 1.8945 | Time(s) 2.1893 | Train_Accu 0.707099 | Test_Accu 0.642524\n",
      "Epoch 034 | Loss 8.9905 | Time(s) 2.1947 | Train_Accu 0.646971 | Test_Accu 0.522328\n",
      "Epoch 035 | Loss 6.3432 | Time(s) 2.1963 | Train_Accu 0.622414 | Test_Accu 0.596312\n",
      "Epoch 036 | Loss 5.7342 | Time(s) 2.2091 | Train_Accu 0.640869 | Test_Accu 0.592979\n",
      "Epoch 037 | Loss 5.1959 | Time(s) 2.2149 | Train_Accu 0.702634 | Test_Accu 0.546767\n",
      "Epoch 038 | Loss 1.9740 | Time(s) 2.2162 | Train_Accu 0.741926 | Test_Accu 0.606310\n",
      "Epoch 039 | Loss 6.1365 | Time(s) 2.2170 | Train_Accu 0.642655 | Test_Accu 0.592979\n",
      "Epoch 040 | Loss 3.5653 | Time(s) 2.2167 | Train_Accu 0.684328 | Test_Accu 0.601644\n",
      "Epoch 041 | Loss 6.1354 | Time(s) 2.2153 | Train_Accu 0.672719 | Test_Accu 0.530993\n",
      "Epoch 042 | Loss 1.8206 | Time(s) 2.2136 | Train_Accu 0.720048 | Test_Accu 0.648967\n",
      "Epoch 043 | Loss 3.5341 | Time(s) 2.2121 | Train_Accu 0.687305 | Test_Accu 0.610531\n",
      "Epoch 044 | Loss 1.6374 | Time(s) 2.2110 | Train_Accu 0.744158 | Test_Accu 0.632304\n",
      "Epoch 045 | Loss 4.5434 | Time(s) 2.2106 | Train_Accu 0.719452 | Test_Accu 0.544768\n",
      "Epoch 046 | Loss 2.7597 | Time(s) 2.2094 | Train_Accu 0.708141 | Test_Accu 0.643413\n",
      "Epoch 047 | Loss 3.3502 | Time(s) 2.2078 | Train_Accu 0.690728 | Test_Accu 0.612308\n",
      "Epoch 048 | Loss 2.0467 | Time(s) 2.2077 | Train_Accu 0.755916 | Test_Accu 0.594757\n",
      "Epoch 049 | Loss 3.0133 | Time(s) 2.2092 | Train_Accu 0.736270 | Test_Accu 0.584092\n"
     ]
    }
   ],
   "source": [
    "dur = []\n",
    "for epoch in range(50):\n",
    "    \n",
    "    t0 = time.time()\n",
    "\n",
    "    pred_prob = net.forward(graph_whole, whole_feature)\n",
    "    loss = criterion( pred_prob[train_mask] , whole_label[train_mask].float() )\n",
    "    \n",
    "    pred_label = net.predict(pred_prob)\n",
    "    train_accu = accuracy_score( pred_label[train_mask] , whole_label[train_mask] )\n",
    "    test_accu = accuracy_score(pred_label[test_mask],whole_label[test_mask])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    dur.append(time.time() - t0)\n",
    "    print(\"Epoch {:03d} | Loss {:.4f} | Time(s) {:.4f} | Train_Accu {:4f} | Test_Accu {:4f}\".format(\n",
    "        epoch, loss.item(), np.mean(dur), train_accu,test_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4691358024691358\n"
     ]
    }
   ],
   "source": [
    "def add_confidence(df,prob,col_name='fact_confidence'):\n",
    "    df[col_name] = None\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i,col_name] = float(prob[i])\n",
    "    return df\n",
    "\n",
    "def sim_Jaccard (str1,str2) :\n",
    "    set1 = set( str1.lower().replace(';',' ').replace(',',' ').replace('.',' ').replace(':',' ').replace('&',' ').\n",
    "               replace('/',' ').replace('\\'',' ').replace('(author)',' ').replace('(joint author)',' ').split() )\n",
    "    set2 = set( str2.lower().replace(';',' ').replace(',',' ').replace('.',' ').replace(':',' ').replace('&',' ').\n",
    "               replace('/',' ').replace('\\'',' ').replace('(author)',' ').replace('(joint author)',' ').split() )\n",
    "    return len(set1&set2)/len(set1|set2)\n",
    "\n",
    "def MV(df,indexK='isbn',answer='author',withWeight=False,weight='confidence'):\n",
    "    df_mv = pd.DataFrame(columns=[indexK,answer])\n",
    "    for indexV in df[indexK].unique():\n",
    "        data_slice = df[df[indexK]==indexV]\n",
    "        vote_dict = {}\n",
    "        for index,row in data_slice.iterrows():\n",
    "            flag = False\n",
    "            for key in vote_dict.keys():\n",
    "                if ( sim_Jaccard(key,row[answer])>=0.8 ):\n",
    "                    flag = True\n",
    "                    if(not withWeight):\n",
    "                        vote_dict[key] += 1\n",
    "                    else:\n",
    "                        vote_dict[key] += float(row[weight])\n",
    "                    break\n",
    "            if (not flag):\n",
    "                if(not withWeight):\n",
    "                    vote_dict[row[answer]] = 1\n",
    "                else:\n",
    "                    vote_dict[row[answer]] = float(row[weight])\n",
    "        vote_list = sorted(vote_dict.items(), key=lambda d:d[1],reverse=True)\n",
    "        #print({indexK:indexV,answer:vote_list[0][0]})\n",
    "        df_mv = df_mv.append({indexK:indexV,answer:vote_list[0][0]},ignore_index=True)\n",
    "    return df_mv\n",
    "\n",
    "def JudgeAccu(label,pred,pred_col='author'):\n",
    "    score = 0\n",
    "    for index,row in pred.iterrows():\n",
    "        if not(index in label.index):\n",
    "            print(index,'no answer')\n",
    "            score += 0 \n",
    "        elif sim_Jaccard(row[pred_col],label.loc[index][pred_col])>=0.8:\n",
    "            score +=1\n",
    "        else:\n",
    "            #print(row[pred_col],\"vs\",label.loc[index][pred_col])\n",
    "            score += 0\n",
    "    return score/len(pred)\n",
    "\n",
    "data_withConfidence = add_confidence(data_golden,F.sigmoid(pred_prob))\n",
    "\n",
    "df_mv = MV(data_withConfidence[test_mask.numpy()],withWeight=True,weight='fact_confidence')\n",
    "df_mv.to_csv( './DataSet/vldbBook/GCNResult.txt' , sep='\\t' , index=False )\n",
    "\n",
    "label = pd.read_csv('./DataSet/vldbBook/book_truth.txt',sep='\\t',low_memory=False,index_col=0)\n",
    "label.rename(columns={'isbn_10':'isbn','authors_truth':'author'},inplace=True)\n",
    "pred = pd.read_csv('./DataSet/vldbBook/GCNResult.txt',sep='\\t',low_memory=False,index_col=0)\n",
    "\n",
    "print(JudgeAccu(label,pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_gpu]",
   "language": "python",
   "name": "conda-env-pytorch_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
